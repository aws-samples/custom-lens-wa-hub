{
    "schemaVersion": "2021-11-01",
    "name": "Amazon Machine Learning - Intelligent Document Process (IDP) Lens",
    "description": "Best Practices on Intelligent Document Process (IDP) Best Practices",
    "_version":"1.0.1",
    "_release_date":"2023-11-09",
    "_release_note":"1) All 6 pillars ready. 2) All Pillar/Question/Choice ID converted to fit with AWS WA Lens Standard",    
    "pillars": [
        {
            "id": "operationalExcellence",
            "name": "Operational Excellence",
            "questions": [
                {
                    "id": "OPS_1",
                    "title": "How does your organization's culture and operating model support the effective design, deployment, and management of your docum",
                    "description": "For workloads centered on document processing automation, it's not just about the tech stack, but also the interplay of the company's culture, internal practices, and procedural frameworks. This evaluation seeks to understand how well AWS methodologies are ingrained within the business, the organization's agility in adapting to changes, and its harmony with the principles of the Well-Architected Framework, with a special emphasis on the organizational aspect",
                    "helpfulResource": {
                        "displayText": "By evaluating the organization's culture and operating model against these best practices, you can identify areas of strength and areas that may need further attention or improvement. The goal is to ensure that the organization is well-positioned not just technically but also culturally and operationally to make the most of the AWS cloud environment for document processing.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/operational-excellence-pillar/organization.html"
                    },
                    "choices": [
                        {
                            "id": "OPS_1_1",
                            "title": "Continuous Training and Upskilling",
                            "helpfulResource": {
                                "displayText": "Our organization champions regular training sessions and encourages certifications, ensuring our teams are updated with the evolving AWS services and best practices."
                            },
                            "improvementPlan": {
                                "displayText": "Periodically reassess roles to ensure relevance. Encourage cross-training to enhance adaptability and foster team collaboration. Initiate feedback mechanisms to identify and rectify any ambiguities"
                            }
                        },
                        {
                            "id": "OPS_1_2",
                            "title": "Feedback-Driven Iterative Improvements",
                            "helpfulResource": {
                                "displayText": "We value feedback from all stakeholders, be it technical teams or end-users. This input helps us in refining our processes, ensuring our document processing system remains agile and responsive to changing needs"
                            },
                            "improvementPlan": {
                                "displayText": "Introduce more flexible learning opportunities like self-paced courses or mentorship programs. Recognize and reward continuous learning endeavors. Allocate resources to support and encourage upskilling."
                            }
                        },
                        {
                            "id": "OPS_1_3",
                            "title": "Alignment with Business Objectives",
                            "helpfulResource": {
                                "displayText": "Our strategies for document processing, are always drafted in line with our business goals, ensuring technological efforts directly contribute to organizational success."
                            },
                            "improvementPlan": {
                                "displayText": "Initiate structured methods for collecting feedback specifically about document processing automation challenges and successes. Regularly revisit and adjust the automation processes based on the feedback. Consider hosting dedicated forums to discuss document processing improvements."
                            }
                        },
                        {
                            "id": "OPS_1_4",
                            "title": "Change Management Processes in Place",
                            "helpfulResource": {
                                "displayText": "Changes in document processing automation workloads can have ripple effects. We've implemented processes to manage these changes, ensuring minimal disruption and maximum stakeholder awareness."
                            },
                            "improvementPlan": {
                                "displayText": "Engage both technical teams and business stakeholders in regular discussions about the evolving needs of document processing. Adjust the automation strategies to cater to both immediate and long-term business objectives. Stay receptive to changing business landscapes, ensuring the automation workload remains adaptable."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_1_1 && OPS_1_2 && OPS_1_3 && OPS_1_4",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_2",
                    "title": "How do you measure the effectiveness of your document processing workflows and continually improve the processes based on these",
                    "description": "To achieve optimal efficiency in document processing, it's imperative to monitor, evaluate, and iterate on workflows. This question delves into your methods of measuring and analyzing the efficacy of your document processing systems. The focus is on understanding the metrics and KPIs you employ and how you harness these insights to refine and elevate your processes. Through continual measurement and refinement, organizations can enhance accuracy, speed, and overall performance in document processing tasks.",
                    "choices": [
                        {
                            "id": "OPS_2_1",
                            "title": "Defined Metrics for IDP Success",
                            "helpfulResource": {
                                "displayText": "Utilizing service-specific metrics allows for a granular understanding of how each component of the IDP workload performs, enabling fine-tuning and optimization. For instance, using CloudWatch, one could monitor the number of documents processed through Textract. Similarly, monitoring the volume and size of documents being uploaded to an S3 bucket can provide insights into the rate at which processing demand is increasing. Further, with AWS Step Functions, the built-in metrics can be leveraged to track the execution success rate, offering insights into the effectiveness of the workflow orchestration."
                            },
                            "improvementPlan": {
                                "displayText": "Benchmark Setting: Establish benchmark metrics based on current Textract extraction accuracy and Comprehend classification or entity recognition scores. Regular Review: Schedule periodic reviews of the extracted data's quality, identifying common issues like misinterpretations or missed text sections.\n\nOptimize Document Quality: Enhance the document quality before processing with Textract. This includes ensuring good resolution, reducing noise, and following other best practices to improve text extraction results.\n\nPost-processing: Implement post-processing checks using bounding box information to validate and correct any inconsistencies in the extracted data."
                            }
                        },
                        {
                            "id": "OPS_2_2",
                            "title": "Feedback Loop from Human Review",
                            "helpfulResource": {
                                "displayText": "Establishing feedback loops from human reviews can serve as a corrective metric for the automation process. When Textract or Comprehend results require human intervention due to low confidence scores, it's an indication of areas where automation might need refining. By measuring the frequency of such interventions, and the reasons behind them, one can iteratively improve the efficiency of the pipeline and training data. This continual feedback refines the automation process, improving accuracy over time."
                            },
                            "improvementPlan": {
                                "displayText": "Human Review Logging: Log every human intervention, particularly noting areas where Textract may have missed content or where Comprehend's analysis was uncertain. Quantitative Analysis: Track patterns in the human review logs. For Textract, this could be certain document types or layouts that are problematic. \n\nDocument Refinement: Based on feedback, refine how documents are prepared for Textract. For instance, if certain layouts are problematic, consider preprocessing adjustments.\n"
                            }
                        },
                        {
                            "id": "OPS_2_3",
                            "title": "Monitoring and Improving Retry Mechanisms",
                            "helpfulResource": {
                                "displayText": "The effectiveness of an IDP workflow is also seen in its ability to handle and recover from errors. By tracking how often retries occur (e.g., when Textract or Comprehend fails, or when there's a glitch in retrieving a document from S3), and the success rate of these retries, one can get insights into system robustness. Metrics derived from Step Functions on the outcomes of these retries can highlight bottlenecks or frequent failure points. Analyzing these metrics allows for refinements in the workflow to improve resilience and reduce bottlenecks."
                            },
                            "improvementPlan": {
                                "displayText": "Error Logging: Ensure all errors are comprehensively logged, whether they're from Textract's extraction process or Comprehend's analysis.\n\nRoot Cause Analysis: Identify any consistent failure points.\n\nDocument Best Practices: Continually update and educate teams on best practices for preparing documents for Textract to reduce extraction errors.\n\nRefined Error Handling: Implement better error handling in the IDP pipeline, with specific responses to known issues. For example, if a certain type of document consistently causes Textract errors, the pipeline could automatically route it for human review."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_2_1 && OPS_2_2 && OPS_2_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_3",
                    "title": "How do you design your document processing workloads so that it can handle varying processing demands and unexpected challenges",
                    "description": "What architectural choices have been made and what strategies are in place for your document processing workloads, emphasizing its adaptability to changing demands and unexpected issues. How does the system scales with varying document volumes, its resilience against disruptions, and its preparedness for unanticipated challenges in the document processing pipeline. Essentially, the emphasis is on the agility, flexibility, and robustness of your setup.",
                    "choices": [
                        {
                            "id": "OPS_3_1",
                            "title": "Throttling Management",
                            "helpfulResource": {
                                "displayText": "Throttling Management refers to the methods and strategies employed to handle rate-limiting imposed by services like Amazon Textract and Amazon Comprehend. AWS services often have limits to prevent misuse or to maintain a quality of service for all users. For Textract and Comprehend, this includes quotas on the number of API calls per second or the size of data being processed. Effective throttling management ensures that these quota limits do not impact your application's performance or user experience, by implementing mechanisms like backoff retries, queueing, and request prioritization."
                            },
                            "improvementPlan": {
                                "displayText": "Monitor Usage Metrics: Utilize Amazon CloudWatch to continuously monitor the number of API calls being made to Textract and Comprehend. This will help you identify potential rate limit breaches before they happen.\n\nImplement Exponential Backoff with Textract: When interfacing with Textract, ensure you manage throttling and dropped connections by setting the Config parameter when creating the Amazon Textract client. It's recommended to set a retry count of 5, as the AWS SDK retries an operation this specified number of times before considering it a failure. Incorporating this mechanism can handle throttling more effectively, using the SDK's built-in exponential backoff strategy.\n\nUse Amazon SQS: Implement Amazon Simple Queue Service (SQS) to queue up requests. This ensures that if there's a surge in demand or if the service starts throttling, the excess requests are queued up and processed in an orderly manner.\n\nPrioritize Critical Requests: If some document processing tasks are more critical than others, implement logic to prioritize these requests so that they are less likely to be throttled.\n\nStay Updated with Service Limits: AWS might periodically update the service limits based on various factors. Stay updated with the latest documentation and adjust your throttling management strategies accordingly.\n\nFeedback Loop with Application Users: Inform users if there's a delay in document processing due to rate limiting. This enhances user experience by setting clear expectations.\n\nConsider Service Quotas Increase: If your application consistently runs into throttling limits, consider requesting AWS to increase your service quotas for Textract and Comprehend."
                            }
                        },
                        {
                            "id": "OPS_3_2",
                            "title": "Rules Specified for Human Review",
                            "helpfulResource": {
                                "displayText": "Setting clear conditions for when to involve humans ensures a balanced approach between automation efficiency and output accuracy. For example, with Textract. if the confidence score for extracting text from a document drops below a threshold, a human review process can be triggered. Similarly, if Comprehend's entity recognition identifies potential sensitive information but doesn't reach a high confidence score, a human verification step can be incorporated using Sagemaker GroudTruth. Using AWS Step Functions, workflows can be designed such that, based on output confidence scores from services like Textract or Comprehend, the flow can diverge to either continue with automation or switch to a human review state."
                            },
                            "improvementPlan": {
                                "displayText": "Establish Clear Confidence Thresholds:Analyze past outputs of Textract and Comprehend to determine optimal confidence score thresholds that balance accuracy with automation. These thresholds dictate when to flag results for human review.\n\nIntegrate Amazon SageMaker GroundTruth: For outputs falling below the confidence score thresholds, utilize Amazon SageMaker GroundTruth to provide a user-friendly interface for human review. This ensures accurate labeling and data verification.\n\nAWS Step Functions for Workflow Automation:Design and implement workflows using AWS Step Functions where, based on the output confidence scores from Textract or Comprehend, the process can smoothly transition between automated steps and human review states.\n\nFeedback Mechanism for Continuous Improvement: As human reviewers correct or verify outputs, gather this feedback to fine-tune and recalibrate confidence thresholds or potentially inform model retraining for services like Comprehend which supports custom models."
                            }
                        },
                        {
                            "id": "OPS_3_3",
                            "title": "Well-designed Re-try Mechanisms for Failures",
                            "helpfulResource": {
                                "displayText": "Ensuring resilience in the face of failures is essential for smooth operations. For instance, if Textract or Comprehend encounters a transient issue while processing a document, using Step Functions, a retry mechanism with exponential backoff can be implemented. Similarly, if a document retrieval from S3 fails, logic can be added to retry the retrieval a set number of times before raising an alert. Within Step Functions, the state machine's design can incorporate error catchers that detail how specific errors should be managed, such as retrying a state, transitioning to an alternative state, or notifying an operator."
                            },
                            "improvementPlan": {
                                "displayText": "Monitor and Refine Retry Thresholds: Continuously monitor the frequency and reasons for retries using Amazon CloudWatch. Based on this data, refine the retry thresholds and limits for services like Textract and Comprehend to ensure optimal performance. For instance, if a particular error type is being retried too frequently without success, it might be more efficient to adjust the retry mechanism or improve the initial processing step.\n\nEnhance Error Logging and Reporting: While Step Functions provide error catchers, enhance the logging mechanism to capture detailed information about the nature of failures, especially those that surpass the retry limits. This would help in diagnosing systemic issues or identifying patterns that could inform future improvements. Integration with Amazon CloudTrail can provide granular visibility into operational activity and any API calls that may fail, aiding in diagnosis.\n\nAutomated Alerts and Notifications:Implement a mechanism where, if certain failures continue beyond a specified threshold, an automated alert is sent to the operations team or relevant stakeholders. Integration with Amazon SNS (Simple Notification Service) can ensure timely notifications, enabling quick action and potentially preventing larger operational disruptions."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_3_1 && OPS_3_2 && OPS_3_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_4",
                    "title": "When designing your document processing workflow using IDP AI services, how does your organization address document quality, an",
                    "description": "Designing an IDP AI services-based workflow involves evaluating document quality and employing pre-processing measures, such as image sharpening and noise reduction, to ensure optimal data input. Post-processing, on the other hand, focuses on refining outputs, like using confidence scores to trigger human reviews or further data manipulations, ensuring the highest accuracy and actionable insights are derived.\n\n\n\n",
                    "choices": [
                        {
                            "id": "OPS_4_2",
                            "title": "Document Quality Optimization for Textract & Comprehend",
                            "helpfulResource": {
                                "displayText": "Before processing documents, we assess document quality in terms of clarity, resolution, and legibility. Documents not meeting our standards are flagged for manual intervention or subjected to enhancement techniques like image sharpening and noise reduction. \n\nPost-processing, we further refine outputs using cross-referencing with bounding box information, filtering out low-confidence results, structuring data for downstream applications, and triggering human reviews when anomalies or certain thresholds are detected"
                            },
                            "improvementPlan": {
                                "displayText": "Pre-processing: Before submitting documents to Textract or Comprehend, it's essential to ensure they are in an optimal format for accurate extraction or comprehension. This might involve techniques such as:\n\nImage enhancement to improve readability.\nNoise reduction to eliminate background artifacts.\nFormat conversion ensuring documents are in a supported format.\nSegmenting larger documents for efficient batch processing\n\nPost-processing: After receiving output from Textract or Comprehend, certain steps enhance or refine the results:\n\nWith Textract, leveraging bounding box information can help map extracted data to its position in the original document, aiding in data validation.\nFiltering out erroneous or low-confidence extractions.\nIntegrating with AWS Step Functions for workflow management, automating steps like human review when confidence scores fall below a certain threshold.\nTransforming raw extracted data into structured formats suitable for downstream applications or storage solutions like Amazon S3."
                            }
                        },
                        {
                            "id": "OPS_4_3",
                            "title": "Collaboration with Domain Experts",
                            "helpfulResource": {
                                "displayText": "Recognizing that some documents have domain-specific nuances, we collaborate with subject matter experts during the post-processing phase. Their insights help in refining and validating the extracted data, ensuring a higher degree of accuracy."
                            },
                            "improvementPlan": {
                                "displayText": "Expand SME Engagement: Diversify the set of subject matter experts (SMEs) to cover a broader spectrum of nuances, ensuring comprehensive understanding and addressing of domain-specific intricacies in documents.\n\nIncorporate Feedback Loops: Establish an iterative system where SMEs can consistently provide inputs, helping to further refine the post-processing algorithms and ensure the extracted data's accuracy.\n\nCombine AI Insights with SME Review: Integrate machine learning insights to direct SME attention to critical sections in documents, optimizing their review process and maximizing data extraction accuracy."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_4_2 && OPS_4_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_5",
                    "title": "How do you ensure operational excellence in your AWS-based document processing workload when interfacing with critical external",
                    "description": "How do you ensure operational excellence in your AWS-based document processing workload when interfacing with critical external systems or records, especially when integration requirements might vary?",
                    "choices": [
                        {
                            "id": "OPS_5_1",
                            "title": "Modular architecture",
                            "helpfulResource": {
                                "displayText": "We implement a modular architecture, leveraging AWS Lambda and AWS Step Functions, that allows us to easily swap or modify integration components without affecting the core processing logic. A modular architecture is fundamental for maintaining flexibility. \n\nBy isolating integration components, we ensure that changes in external systems or records have minimal impact on the main document processing workflow. Using AWS Lambda provides granular control and scalability, while AWS Step Functions offers reliable state management and orchestration for complex integrations."
                            },
                            "improvementPlan": {
                                "displayText": "Evaluate Current Architecture: Review the existing architecture for modularity. Identify components that handle external system integrations.\n\nRefactor using AWS Lambda: Break down integration logic into smaller, granular functions using AWS Lambda for flexibility and scalability.\n\nImplement AWS Step Functions: Design workflows using AWS Step Functions that define the sequence of Lambda functions, ensuring smooth transitions between different integration components.\n\nTest: Continuously test the modular components to ensure they can be easily swapped or modified without affecting core logic.\n\nFeedback Loop: Continuously seek feedback from developers and integration partners to refine and optimize the architecture."
                            }
                        },
                        {
                            "id": "OPS_5_2",
                            "title": "Event Driven/Looseely coupled",
                            "helpfulResource": {
                                "displayText": "We utilize AWS EventBridge for capturing and routing events from external systems, ensuring decoupled integrations and streamlined event-driven processing.\n\nAWS EventBridge provides a serverless event bus that simplifies the connection between applications. Its event-driven nature is crucial for real-time responsiveness and operational excellence. Decoupling the core processing from external system events ensures stability and seamless handling of varying integration requirements.\n"
                            },
                            "improvementPlan": {
                                "displayText": "Identify Event Sources: List all external systems and identify potential event sources.\n\nSet Up AWS EventBridge: Integrate these sources with AWS EventBridge and configure routing rules for various events.\n\nOptimize Event Handling: Ensure that the document processing workload is equipped to handle these events efficiently.\n\nMonitor and Adjust: Monitor event traffic and adjust routing rules as required to improve performance and responsiveness.\n\nRegular Reviews: Conduct periodic reviews of integrations to cater to evolving requirements."
                            }
                        },
                        {
                            "id": "OPS_5_3",
                            "title": "Secrets Management",
                            "helpfulResource": {
                                "displayText": "We employ AWS Secrets Manager and AWS Parameter Store for secure management of credentials and configuration parameters related to external systems, ensuring consistent and secure access.\n\nMaintaining the security and consistency of integrations is paramount. AWS Secrets Manager and Parameter Store are vital tools to securely store, retrieve, and manage secrets and configurations, thus ensuring seamless integration with external systems without compromising security.\n"
                            },
                            "improvementPlan": {
                                "displayText": "Inventory of Credentials: Enumerate all credentials and configuration parameters related to external systems.\n\nMigrate to AWS Secrets Manager: Migrate sensitive credentials to AWS Secrets Manager, ensuring they're encrypted and securely retrievable.\n\nUse AWS Parameter Store: Store configuration parameters in AWS Parameter Store.\n\nAccess Management: Regularly review and manage access to these secrets and parameters, granting permissions only to necessary entities.\n\nAudit and Rotate: Periodically audit access logs and rotate secrets to maintain security."
                            }
                        },
                        {
                            "id": "OPS_5_4",
                            "title": "Mock integration test",
                            "helpfulResource": {
                                "displayText": "We conduct regular mock integrations and tests using Amazon API Gateway and AWS Lambda, simulating external system behaviors to proactively identify and mitigate potential integration issues\n\nRegular testing against mock versions of external systems ensures that the integration logic remains robust and adaptable. Using Amazon API Gateway for creating mock endpoints and AWS Lambda for simulation logic offers a scalable and flexible testing environment, essential for operational excellence."
                            },
                            "improvementPlan": {
                                "displayText": "Identify Integration Points: Highlight all integration points with external systems.\n\nDevelop Mocks using Amazon API Gateway: For each integration point, develop a mock endpoint using Amazon API Gateway.\n\nSimulate External Behavior with AWS Lambda: Design AWS Lambda functions to simulate external system behaviors.\n\nRegular Testing: Schedule routine tests against these mock integrations to validate the system's resilience and adaptability.\n\nIterate Based on Results: Refine integration logic based on test outcomes and discovered inefficiencies."
                            }
                        },
                        {
                            "id": "OPS_5_5",
                            "title": "Documentation",
                            "helpfulResource": {
                                "displayText": "We maintain comprehensive documentation and have established communication protocols with stakeholders of external systems to stay updated on potential changes, ensuring proactive adaptation.\n\nOperational excellence is not just about technical solutions. Open communication and thorough documentation play a pivotal role in ensuring smooth integrations with varying external systems. Being proactive and staying informed about potential changes allows for timely adjustments, minimizing disruptions."
                            },
                            "improvementPlan": {
                                "displayText": "Document Existing Integrations: Thoroughly document the nature, requirements, and behaviors of all current integrations.\n\nEstablish Communication Protocols: Set up structured communication channels with stakeholders of external systems.\n\nStay Informed: Regularly check for updates or changes in external systems that might affect integrations.\n\nProactive Adaptation: Develop a plan for rapid adaptation and change implementation upon receiving updates about external system changes.\n\nFeedback Collection: Encourage stakeholders to provide feedback on the integration process to continually refine and improve."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_5_1 && OPS_5_2 && OPS_5_3 && OPS_5_4 && OPS_5_5",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_6",
                    "title": "How does your organization optimize the build and release processes for document processing workloads?",
                    "description": "Optimizing build and release processes are critical for ensuring agile, efficient, and robust deployments of document processing workloads that often demand high accuracy and reliability. This question evaluates how organizations approach this crucial step, leveraging AWS services and best practices, all in line with the AWS Well-Architected Framework's principles.\n",
                    "choices": [
                        {
                            "id": "OPS_6_1",
                            "title": "Utilization of Infrastructure as Code (IaC)",
                            "helpfulResource": {
                                "displayText": "We leverage AWS services such as CloudFormation or the AWS CDK to define and provision our document processing resources. This ensures consistent, reproducible, and scalable infrastructure setups, enabling smoother build and release processes."
                            },
                            "improvementPlan": {
                                "displayText": "-"
                            }
                        },
                        {
                            "id": "OPS_6_2",
                            "title": "Continuous Integration and Continuous Deployment (CI/CD) Practices",
                            "helpfulResource": {
                                "displayText": "We use AWS services like AWS CodePipeline and AWS CodeBuild for our CI/CD workflows. This ensures continuous integration and seamless deployment of document processing workloads, minimizing manual errors and reducing deployment times."
                            },
                            "improvementPlan": {
                                "displayText": "-"
                            }
                        },
                        {
                            "id": "OPS_6_3",
                            "title": "Automated Testing and Quality Assurance",
                            "helpfulResource": {
                                "displayText": "Using AWS tools like AWS CodeStar and integration with third-party solutions, we ensure automated testing of our document processing applications. Every build undergoes rigorous testing to validate functionality, performance, and security, ensuring high-quality releases."
                            },
                            "improvementPlan": {
                                "displayText": "-"
                            }
                        },
                        {
                            "id": "OPS_6_4",
                            "title": "Data Security and Compliance",
                            "helpfulResource": {
                                "displayText": "We use AWS tools and best practices, like IAM roles and policies, to ensure that our interaction with Textract, Comprehend, and other services is secure. Additionally, we're aware of data retention and compliance needs, ensuring that processed documents are handled appropriately."
                            },
                            "improvementPlan": {
                                "displayText": "-"
                            }
                        },
                        {
                            "id": "OPS_6_5",
                            "title": "Seamless Integration with Diverse External Systems",
                            "helpfulResource": {
                                "displayText": "We have tailored our build and release pipelines to emphasize seamless integration with diverse external systems. Leveraging AWS services and best practices, our document processing workflows are designed to easily interface and adapt to various external requirements. This ensures consistency and agility in deployments, prioritizing operational excellence even in complex integration scenarios."
                            },
                            "improvementPlan": {
                                "displayText": "-"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_6_1 && OPS_6_2 && OPS_6_3 && OPS_6_4 && OPS_6_5",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_7",
                    "title": "How do you orchestrate tasks associated with training and evaluating new custom model versions for optimized Build & Release pr",
                    "description": "Optimizing build and release processes are critical for ensuring agile, efficient, and robust deployments of document processing workloads that often demand high accuracy and reliability. This question evaluates how organizations approach this crucial step, leveraging AWS services and best practices, all in line with the AWS Well-Architected Framework's principles.",
                    "choices": [
                        {
                            "id": "OPS_7_1",
                            "title": "Utilizing ML Flywheel for Model Versioning",
                            "helpfulResource": {
                                "displayText": "We leverage the capabilities of Amazon Comprehend's Flywheel to orchestrate the tasks related to training and evaluating new versions of our custom models. This streamlined approach allows for efficient model updates, ensuring our document processing solutions stay accurate and updated. The use of Flywheel, combined with AWS CloudFormation, facilitates consistent and reproducible deployment of model versions"
                            },
                            "improvementPlan": {
                                "displayText": "Continuous Model Evaluation: Integrate continuous evaluation tools that constantly feed real-world, post-deployment data back to the Flywheel. This will help in identifying drifts in model performance, triggering re-training sessions, and deploying updated models more proactively.\n\nEnhanced Version Management:Collaborate with the DevOps team to ensure that every model version deployed via AWS CloudFormation has a clear version tag, enabling swift rollback to previous versions if required. Maintaining a changelog for each version will also aid in tracking performance improvements or regressions over time.\n\nFeedback Loop Integration: Establish a feedback mechanism wherein end-users or domain experts can provide input on model predictions. Integrating this feedback into the Flywheel can refine the training and evaluation processes, further enhancing the model's accuracy and relevance to real-world scenarios."
                            }
                        },
                        {
                            "id": "OPS_7_2",
                            "title": "Business-centric Approach in MLOps",
                            "helpfulResource": {
                                "displayText": "Our MLOps strategy is fundamentally aligned with our business goals. By focusing on the broader objectives, we ensure that all ML deployments, including those integrated through Comprehend's Flywheel, directly contribute to driving business valuebe it opening new use-case avenues, enhancing team productivity, or minimizing operational overheads."
                            },
                            "improvementPlan": {
                                "displayText": "Align MLOps and Business Teams: Frequently hold alignment sessions between the MLOps and business teams to ensure that the ML developments are in tandem with evolving business goals.\n\nPerformance Metrics Integration: Establish Key Performance Indicators (KPIs) for ML models that directly map to business outcomes. Regularly track and optimize these KPIs in line with business objectives.\n\nPeriodic Business Impact Assessment: Schedule periodic reviews to assess the direct and indirect impact of ML deployments on business outcomes. Adjust strategies based on the insights gathered."
                            }
                        },
                        {
                            "id": "OPS_7_3",
                            "title": "Modular Implementation of MLOps Processes",
                            "helpfulResource": {
                                "displayText": "We structure our MLOps processes modularly, emphasizing code quality and clear demarcation of different ML lifecycle phases. This modular approach, resonating with Amazon Comprehend's Flywheel philosophy, ensures ease in testing, enhances maintainability, and allows for efficient code refactoring. Each phase, from training to deployment, is treated as a distinct module, optimizing the build and release sequences."
                            },
                            "improvementPlan": {
                                "displayText": "Enhanced Component Testing: Emphasize testing of each modular component in isolation, using unit tests and integration tests, to ensure they function correctly both standalone and in sequence.\n\nDocumentation & Collaboration: Encourage teams to maintain thorough documentation for each module, and foster cross-team collaboration to understand dependencies and impacts better.\n\nCode Quality Audits: Periodically perform quality audits and refactoring sessions to ensure that the modular approach remains efficient and isn't becoming unwieldy or overly complex."
                            }
                        },
                        {
                            "id": "OPS_7_4",
                            "title": "Defining Steps and Responsibilities in ML Workflow",
                            "helpfulResource": {
                                "displayText": "In our document processing workflow, distinct steps such as training, evaluation, and deployment are clearly defined. Moreover, we've categorized these into super-steps, like the end-to-end training-to-deployment pipeline, to ensure clarity. This structure, inspired by Amazon Comprehend's Flywheel, helps in assigning clear responsibilities and ensures that our ML applications remain agile and adaptable."
                            },
                            "improvementPlan": {
                                "displayText": "Role-based Training: Offer training sessions tailored to each role within the ML workflow, ensuring everyone is aware of their responsibilities and how their tasks fit within the bigger picture.\n\nFeedback Loops: Establish feedback loops for every step in the workflow. This will enable teams to communicate challenges, successes, and areas of improvement after each release or model iteration.\n\nWorkflow Automation & Monitoring: Use tools like AWS Step Functions to automate the workflow wherever possible. Also, integrate monitoring tools to keep track of each step's performance, flagging any inconsistencies or bottlenecks for timely resolution."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_7_1 && OPS_7_2 && OPS_7_3 && OPS_7_4",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_8",
                    "title": "Considering the AWS Well-Architected Review for Operational Excellence, how have you integrated Infrastructure as Code (IaC) pr",
                    "description": "The Intelligent Document Processing Architecture capitalizes on a serverless infrastructure, offering a robust solution for document-related tasks. Implementing this architecture with Infrastructure as Code (IaC) not only ensures fast and consistent deployments but also allows for rapid iteration. Employing tools like the CDK for deployment and orchestrating with low-code visual workflows like AWS Step Functions can revolutionize the automation of your development pipeline.",
                    "choices": [
                        {
                            "id": "OPS_8_1",
                            "title": "Fully Integrated with IaC",
                            "helpfulResource": {
                                "displayText": "We have seamlessly incorporated the Intelligent Document Processing Architecture using Infrastructure as Code, utilizing tools like CDK for deployment and AWS Step Functions for orchestration. This approach ensures rapid iteration and consistently reliable deployments in our environment."
                            },
                            "improvementPlan": {
                                "displayText": "Regularly review and update the IaC scripts to incorporate the latest best practices and to adapt to changes in the Intelligent Document Processing Architecture."
                            }
                        },
                        {
                            "id": "OPS_8_2",
                            "title": "Partial Integration with IaC",
                            "helpfulResource": {
                                "displayText": "We have initiated the implementation of the Intelligent Document Processing Architecture using IaC principles. Certain components or phases use the CDK for deployment and AWS Step Functions for orchestration, but the integration is not yet comprehensive across all projects."
                            },
                            "improvementPlan": {
                                "displayText": "Gap Analysis: Identify the components or phases that haven't been integrated with IaC and prioritize them based on business needs.\nExpand Tool Utilization: Start gradually incorporating tools like CDK and AWS Step Functions in areas where they haven't been applied.\nKnowledge Sharing: Organize sessions where teams that have successfully integrated IaC can share their experiences and best practices with those that haven't."
                            }
                        },
                        {
                            "id": "OPS_8_3",
                            "title": "Planning to Integrate with IaC",
                            "helpfulResource": {
                                "displayText": "Recognizing the potential benefits, we are in the preparatory stages of incorporating IaC practices with the Intelligent Document Processing Architecture. Our goal is to leverage tools like CDK and AWS Step Functions to maximize automation and consistency."
                            },
                            "improvementPlan": {
                                "displayText": "Pilot Project: Begin with a small project or component to test the integration of IaC, using CDK and AWS Step Functions.\nTraining: Provide foundational training sessions on IaC principles, CDK, and AWS Step Functions.\nFeedback Loop: As you start the integration, create a feedback mechanism to learn from early challenges and refine the approach."
                            }
                        },
                        {
                            "id": "OPS_8_4",
                            "title": "Limited or No Integration with IaC",
                            "helpfulResource": {
                                "displayText": "Our deployment of the Intelligent Document Processing Architecture currently relies on traditional or manual deployment methods. We have not extensively leveraged IaC practices, CDK, or AWS Step Functions in this context"
                            },
                            "improvementPlan": {
                                "displayText": "Awareness Sessions: Conduct sessions highlighting the benefits and importance of IaC in modern deployment processes.\nStart Small: Initiate the transition to IaC by converting a small, non-critical piece of infrastructure first, learning from the experience.\nExpert Consultation: Consider bringing in experts or consultants who can guide the team on best practices for integrating IaC with the Intelligent Document Processing Architecture."
                            }
                        },
                        {
                            "id": "OPS_8_5",
                            "title": "Unaware of IaC Integration Benefits",
                            "helpfulResource": {
                                "displayText": "We have not explored or are unaware of the advantages that IaC practices, combined with tools like CDK and AWS Step Functions, can bring to the Intelligent Document Processing Architecture deployment."
                            },
                            "improvementPlan": {
                                "displayText": "Educational Workshops: Host workshops on the foundational concepts of IaC and its relevance in today's cloud architectures.\nCase Studies: Share success stories and case studies of organizations that have benefited from integrating IaC into their deployment strategies.\nRoadmap Creation: Collaborate with the team to create a phased roadmap for introducing and integrating IaC practices, CDK, and AWS Step Functions into the current processes."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_8_1 && OPS_8_2 && OPS_8_3 && OPS_8_4 && OPS_8_5",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_9",
                    "title": "How do you ensure effective observability in your document processing workload?",
                    "description": "For document processing workloads, observability is crucial to maintain the consistent quality of processed data, ensure timely processing, and detect anomalies or bottlenecks in the workflow. Given the data-intensive nature of these workloads, monitoring, alerting, and logging become paramount.",
                    "choices": [
                        {
                            "id": "OPS_9_1",
                            "title": "Continuous Monitoring",
                            "helpfulResource": {
                                "displayText": "We monitor real-time transaction rates, response times, and error counts for Textract & Comprehend using CloudWatch."
                            },
                            "improvementPlan": {
                                "displayText": "Regularly update the CloudWatch dashboard to include all relevant metrics from Textract and Comprehend. Ensure that the monitoring scope covers all stages of document processing."
                            }
                        },
                        {
                            "id": "OPS_9_2",
                            "title": "Alerting Mechanisms",
                            "helpfulResource": {
                                "displayText": "We have CloudWatch Alarms set up for anomalies or processing delays in Textract and Comprehend."
                            },
                            "improvementPlan": {
                                "displayText": "Periodically review CloudWatch Alarms to ensure they align with the current operational needs. With these alarms, teams can get notified about anomalies such as unexpected surges in processing times or service rate limits, allowing for proactive response."
                            }
                        },
                        {
                            "id": "OPS_9_3",
                            "title": "Detailed Logging",
                            "helpfulResource": {
                                "displayText": "We capture all API calls to Textract and Comprehend using CloudTrail for comprehensive logging."
                            },
                            "improvementPlan": {
                                "displayText": "CloudTrail provides a transparent record of all interactions with Textract and Comprehend, which aids in security, troubleshooting, and service usage patterns."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_9_1 && OPS_9_2 && OPS_9_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "OPS_10",
                    "title": "How do you observe and manage transactional service quotas for your document processing workload with Textract and Comprehend?",
                    "description": "Managing transactional service quotas in AWS services like Textract is essential. By monitoring metrics such as response times, server errors, and throttled requests, you can optimize document processing workflows and handle traffic spikes efficiently.",
                    "choices": [
                        {
                            "id": "OPS_10_1",
                            "title": "Active Monitoring",
                            "helpfulResource": {
                                "displayText": "We actively monitor service quotas for Textract and Comprehend using the AWS Service Quotas dashboard."
                            },
                            "improvementPlan": {
                                "displayText": "AWS Service Quotas Dashboard: The dashboard provides a centralized view of all quotas across AWS services. Regularly monitoring this ensures that you're not nearing or exceeding any limits."
                            }
                        },
                        {
                            "id": "OPS_10_2",
                            "title": "Responsive architecture",
                            "helpfulResource": {
                                "displayText": "By dynamically scaling or throttling requests based on current rates, we operate smoothly even during traffic spikes."
                            },
                            "improvementPlan": {
                                "displayText": "Invest in optimizing auto-scaling parameters or introduce more efficient request routing to handle traffic bursts"
                            }
                        },
                        {
                            "id": "OPS_10_3",
                            "title": "Adaptive Retry Mechanisms",
                            "helpfulResource": {
                                "displayText": "Instead of failing outright when a quota is exceeded, we have retry mechanisms embedded into the design"
                            },
                            "improvementPlan": {
                                "displayText": "Based on feedback, enhance the retry mechanism to ensure graceful handling of quota exceedances, and minimize impact on end-users."
                            }
                        },
                        {
                            "id": "OPS_10_4",
                            "title": "Load Testing",
                            "helpfulResource": {
                                "displayText": "We simulate high transactional loads, to identify bottlenecks and understand how the system behaves near or at quota limits."
                            },
                            "improvementPlan": {
                                "displayText": "Increase the frequency or variety of load testing scenarios to continually ensure the resilience of the document processing workload as it nears service quotas."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "OPS_10_1 && OPS_10_2 && OPS_10_3 && OPS_10_4",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                }
            ]
        },
        {
            "id": "security",
            "name": "Security",
            "questions": [
                {
                    "id": "SEC_1",
                    "title": "Do you access S3, Textract and Comprehend via VPC endpoints in your IDP workflow?",
                    "description": "You can use Amazon Textract, Comprehend and S3 APIs through the public internet or keeping your network traffic within the AWS network via using VPC endpoints",
                    "helpfulResource": {
                        "displayText": "Consider put your IDP workflow in your VPC while access to Textract, Comprehend, and S3 APIs via VPC endpoints to secure the IDP workflow",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/vpc-interface-endpoints.htm"
                    },
                    "choices": [
                        {
                            "id": "SEC_1_1",
                            "title": "Use VPC endpoints to establish private connection with Amazon Textract, Comprehend and S3",
                            "helpfulResource": {
                                "displayText": "Instead of using the public internet, establish private and secure connections by keeping your network traffic within the AWS network and VPC endpoints to Amazon Comprehend, Textract and S3.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/vpc-interface-endpoints.html"
                            },
                            "improvementPlan": {
                                "displayText": "You can establish a private connection between your VPC and Textract/Comprehend by creating an interface VPC endpoint. You can also access Amazon S3 from your VPC using gateway VPC endpoints.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/securing-amazon-comprehend-api-calls-with-aws-privatelink/"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend and interface VPC endpoints (AWS PrivateLink)",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/vpc-interface-endpoints.html"
                                    },
                                    {
                                      "displayText": "Gateway endpoints for Amazon S3",
                                      "url": "https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "AWS Machine Learning Blog: Using Amazon Textract with AWS PrivateLink",
                                      "url": "https://aws.amazon.com/blogs/machine-learning/using-amazon-textract-with-aws-privatelink/"
                                    },
                                    {
                                      "displayText": "AWS Storage Blog: Managing Amazon S3 access with VPC endpoints and S3 Access Points",
                                      "url": "https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc-endpoints-and-s3-access-points/"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_1_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_2",
                    "title": "Do you have a centralized identity provider for workforce identities?",
                    "description": "For workforce identities to your IDP application, rely on an identity provider that allows you to manage identities in a centralized place.",
                    "helpfulResource": {
                        "displayText": "For workforce identities to your IDP application, rely on an identity provider that allows you to manage identities in a centralized place.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_identity_provider.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_2_1",
                            "title": "Set up a centralized identity provider",
                            "helpfulResource": {
                                "displayText": "Setting up a centralized identity provider makes it simpler to manage access across multiple IDP applications and services. This reduces the need for multiple credentials and provides an opportunity to integrate with existing human resources (HR) processes.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_identities_identity_provider.html"
                            },
                            "improvementPlan": {
                                "displayText": "For federation with individual AWS accounts, you can use centralized identities for AWS with a SAML 2.0-based provider with AWS Identity and Access Management. For federation to multiple accounts in your AWS Organizations, you can configure your identity source in AWS IAM Identity Center and specify where your users and groups are stored.",
                                "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html"
                                
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Manage identities in IAM Identity Center",
                                      "url": "https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-identity-source-sso.html"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_2_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_3",
                    "title": "How do you manage and control user access to IDP services?",
                    "description": "Do you use IAM roles to control user access and do you enforce least privilege access to services in your IDP application?",
                    "helpfulResource": {
                        "displayText": "Do you use IAM roles to control user access and do you enforce least privilege access to services in your IDP application?",
                        "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/access.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_3_1",
                            "title": "Use IAM roles to control user access and enforce least privilege access via IAM policies and tags",
                            "helpfulResource": {
                                "displayText": "Create IAM roles for user access to services in IDP application and attach appropriate policies together with tags to achieve least privilege access",
                                "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/access.html"
                            },
                            "improvementPlan": {
                                "displayText": "Create IAM roles for user access to services in IDP application and attach appropriate policies together with tags to achieve least privilege access",
                                "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_controlling.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Controlling access to AWS resources using tags",
                                      "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_3_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_4",
                    "title": "How do you protect Amazon Textract and Comprehend in your account from cross-service impersonation?",
                    "description": "An IDP application usually has multiple services. One service may call another service. You need to prevent cross-service confused deputy.",
                    "helpfulResource": {
                        "displayText": "Cross-service impersonation can occur when one service calls another service. The calling service can be manipulated to act on another customer's resources even though it shouldn't have the proper permissions, resulting in the confused deputy problem.",
                        "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_4_1",
                            "title": "Use the aws:SourceArn and aws:SourceAccount global condition context keys in resource policies",
                            "helpfulResource": {
                                "displayText": "We recommend using the aws:SourceArn and aws:SourceAccount global condition context keys in resource policies to limit the permissions that Textract or Comprehend gives another service to the resource.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/cross-service-confused-deputy-prevention.html"
                            },
                            "improvementPlan": {
                                "displayText": "We recommend using the aws:SourceArn and aws:SourceAccount global condition context keys in resource policies to limit the permissions that one service gives another service to the resource. If you use both global condition context keys, the aws:SourceAccount value and the account in the aws:SourceArn value must use the same account ID when used in the same policy statement.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/cross-service-confused-deputy-prevention.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_4_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_5",
                    "title": "How do you secure sensitive data?",
                    "description": "IDP process usually involves multiple data stores. Sensitive data in these data stores needs to be secured",
                    "helpfulResource": {
                        "displayText": "These sensitivie data may exist in s3, DynamoDB, or RDS, etc",
                        "url": "https://docs.aws.amazon.com/whitepapers/latest/best-practices-building-data-lake-for-games/data-security-and-governance.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_5_1",
                            "title": "Follow the best practices to secure sensitive data in data stores",
                            "helpfulResource": {
                                "displayText": "The security best practices tie back to some of the key capabilities including defining IAM controls, multiple ways to implement detective controls on databases, strengthening infrastructure security surrounding your data via network flow control, and data protection through encryption and tokenization.",
                                "url": "https://aws.amazon.com/blogs/database/best-practices-for-securing-sensitive-data-in-aws-data-stores/"
                            },
                            "improvementPlan": {
                                "displayText": "Consider defining IAM controls, implementing detective controls on databases, strengthening infrastructure security surrounding your data via network flow control, and/or data protection through encryption and tokenization",
                                "url": "https://aws.amazon.com/blogs/database/best-practices-for-securing-sensitive-data-in-aws-data-stores/"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_5_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_6",
                    "title": "How do you encrypt data at rest in Amazon Textract?",
                    "description": "Data encryption refers to protecting data while in transit and at rest. Amazon Textract uses Transport Layer Security (TLS) and VPC endpoints to encrypt data in transit. But how do you encrypt data at rest?",
                    "helpfulResource": {
                        "displayText": "Data encryption refers to protecting data while in transit and at rest. Amazon Textract uses Transport Layer Security (TLS) and VPC endpoints to encrypt data in transit. You need to choose a way to encrypt data at rest",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/encryption.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_6_1",
                            "title": "Encrypt data at rest with SSE-S3",
                            "helpfulResource": {
                                "displayText": "The primary method of encrypting data at rest in Amazon Textract is server-side encryption. You can do server-side encryption with Amazon S3-Managed Keys (SSE-S3)",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/encryption.html"
                            },
                            "improvementPlan": {
                                "displayText": "When you use SSE-S3, each object is encrypted with a unique key. As an additional safeguard, this method encrypts the key itself with a master key that it regularly rotates.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/encryption.html"
                            }
                        },
                        {
                            "id": "SEC_6_2",
                            "title": "Encrypt data at rest with SSE-KMS",
                            "helpfulResource": {
                                "displayText": "The primary method of encrypting data at rest in Amazon Textract is server-side encryption. You can do server-side encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS)",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/encryption.html"
                            },
                            "improvementPlan": {
                                "displayText": "SSE-KMS has some additional benefits and charges. There are separate permissions for the use of a KMS key that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your KMS key was used and by whom. Additionally, you can create and manage KMS keys or use AWS managed keys that are unique to you, your service, and your Region.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/encryption.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_6_1 && SEC_6_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_7",
                    "title": "Do you encrypt output of Amazon Textract asynchronous API in custom S3 bucket?",
                    "description": "When you call Textract's asynchronous API, the result can be output to a S3 bucket. Do you encrypt that output?",
                    "helpfulResource": {
                        "displayText": "When you call Textract's asynchronous API, the result can be output to a S3 bucket. Do you encrypt that output?",
                        "url": "https://aws.amazon.com/about-aws/whats-new/2020/11/amazon-textract-supports-aws-kms/"
                    },
                    "choices": [
                        {
                            "id": "SEC_7_1",
                            "title": "Specify output S3 bucket and use AWS KMS key",
                            "helpfulResource": {
                                "displayText": "When you start an Amazon Textract job by calling StartDocumentTextDetection or StartDocumentAnalysis, you can specify the S3 bucket for storing the output and specify the AWS KMS customer master key (CMK) to encrypt the output.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/store-output-in-custom-amazon-s3-bucket-and-encrypt-using-aws-kms-for-multi-page-document-processing-with-amazon-textract/"
                            },
                            "improvementPlan": {
                                "displayText": "When you start an Amazon Textract job by calling StartDocumentTextDetection or StartDocumentAnalysis, an optional parameter in the API action is called OutputConfig. This parameter allows you to specify the S3 bucket for storing the output. Another optional input parameter KMSKeyId allows you to specify the AWS KMS customer master key (CMK) to use to encrypt the output.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/store-output-in-custom-amazon-s3-bucket-and-encrypt-using-aws-kms-for-multi-page-document-processing-with-amazon-textract/"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_7_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_8",
                    "title": "Do you leverage KMS encryption in Amazon Comprehend?",
                    "description": "Amazon Comprehend works with AWS KMS to provide enhanced encryption for your data.",
                    "helpfulResource": {
                        "displayText": "Amazon Comprehend works with AWS Key Management Service (AWS KMS) to provide enhanced encryption for your data.",
                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/kms-in-comprehend.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_8_1",
                            "title": "Leverage KMS encryption in Amazon Comprehend to enhance data encryption for Start and Create jobs",
                            "helpfulResource": {
                                "displayText": "Amazon Comprehend works with AWS KMS to provide enhanced encryption for your data.Integration with AWS KMS enables you to encrypt the data in the storage volume for Start and Create jobs, and it encrypts the output results of Start jobs using your own KMS key.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/kms-in-comprehend.html"
                            },
                            "improvementPlan": {
                                "displayText": "For the AWS Management Console, Amazon Comprehend encrypts custom models with its own KMS key. For the AWS CLI, Amazon Comprehend can encrypt custom models using either its own KMS key or a provided customer managed key (CMK). All Amazon Comprehend Start and Create API operations support KMS encrypted input documents.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/kms-in-comprehend.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_8_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_9",
                    "title": "How do you protect PII in IDP output?",
                    "description": "For documents including Personal Identifying Information (PII), the PII from IDP output needs to be protected",
                    "helpfulResource": {
                        "displayText": "Depending on whether or not you need to store the PII in your IDP workflow's downstream, there are different ways to protect the PII.",
                        "url": "https://docs.aws.amazon.com/whitepapers/latest/best-practices-building-data-lake-for-games/data-security-and-governance.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_9_1",
                            "title": "Secure the output PII in your data store",
                            "helpfulResource": {
                                "displayText": "If you need to store the PII in your IDP downstream, follow SEC01-BP04.",
                                "url": "https://aws.amazon.com/blogs/database/best-practices-for-securing-sensitive-data-in-aws-data-stores/"
                            },
                            "improvementPlan": {
                                "displayText": "Consider defining IAM controls, implementing detective controls on databases, strengthening infrastructure security surrounding your data via network flow control, and/or data protection through encryption and tokenization",
                                "url": "https://aws.amazon.com/blogs/database/best-practices-for-securing-sensitive-data-in-aws-data-stores/"
                            }
                        },
                        {
                            "id": "SEC_9_2",
                            "title": "Redact the PII in your IDP output",
                            "helpfulResource": {
                                "displayText": "If you do not need to store the PII in your IDP downstream, consider redacting the PII in your IDP output.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/detecting-and-redacting-pii-using-amazon-comprehend/"
                            },
                            "improvementPlan": {
                                "displayText": "Design a PII redation step using Amazon Comprehend in your IDP workflow ",
                                "url": "https://aws.amazon.com/blogs/machine-learning/detecting-and-redacting-pii-using-amazon-comprehend/"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_9_1 && SEC_9_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_10",
                    "title": "Do you implement secure key management?",
                    "description": "You need to define an encryption approach that includes the storage, rotation, and access control of keys, which helps provide protection for your content",
                    "helpfulResource": {
                        "displayText": "By defining an encryption approach that includes the storage, rotation, and access control of keys, you can help provide protection for your content against unauthorized users and against unnecessary exposure to authorized users.",
                        "url": "https://docs.aws.amazon.com/kms/latest/developerguide/overview.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_10_1",
                            "title": "Use AWS Key Management Service (KMS)",
                            "helpfulResource": {
                                "displayText": "AWS KMS helps you manage encryption keys and integrates with many AWS services. It provides durable, secure, and redundant storage for your AWS KMS keys.",
                                "url": "https://docs.aws.amazon.com/kms/latest/developerguide/overview.html"
                            },
                            "improvementPlan": {
                                "displayText": "You can define your key aliases as well as key-level policies. The policies help you define key administrators as well as key users. You can use the AWS KMS API to create and manage KMS keys and special features, such as custom key stores, and use KMS keys in cryptographic operations.",
                                "url": "https://docs.aws.amazon.com/pdfs/wellarchitected/latest/security-pillar/wellarchitected-security-pillar.pdf#welcome"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_10_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_11",
                    "title": "Do you have secrets management?",
                    "description": "How do you manage secrets, such as API keys in your document processing workflow?",
                    "helpfulResource": {
                        "displayText": "An IDP workflow may have secrets such as API keys in multiple services or stages, you need to have a tool to store, manage, retrieve, and rotate these secrets",
                        "url": "https://aws.amazon.com/blogs/aws/aws-secrets-manager-store-distribute-and-rotate-credentials-securely/"
                    },
                    "choices": [
                        {
                            "id": "SEC_11_1",
                            "title": "Use AWS Secrets Manager",
                            "helpfulResource": {
                                "displayText": "AWS Secrets Manager helps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycles. Storing the credentials in Secrets Manager helps avoid possible compromise by anyone who can inspect your application or the components.",
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"
                            },
                            "improvementPlan": {
                                "displayText": "With Secrets Manager, you can configure an automatic rotation schedule for your secrets. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise. Since the credentials are no longer stored with the application, rotating credentials no longer requires updating your applications and deploying changes to application clients.",
                                "url": "https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_11_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_12",
                    "title": "Do you separate workloads using different accounts?",
                    "description": "If you have multiple IDP workloads, do you separate them using different accounts?",
                    "helpfulResource": {
                        "displayText": "If you have multiple IDP workloads, do you separate them using different accounts?",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_multi_accounts.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_12_1",
                            "title": "Separate workloads using accounts",
                            "helpfulResource": {
                                "displayText": "Establish common guardrails and isolation between environments (such as production, development, and test) and workloads through a multi-account strategy. Account-level separation is strongly recommended, as it provides a strong isolation boundary for security, billing, and access.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_multi_accounts.html"
                            },
                            "improvementPlan": {
                                "displayText": "AWS provides tools to manage your cloud workloads at scale through a multi-account strategy to leverage this isolation boundary.When you have multiple AWS accounts under central management, your accounts should be organized into a hierarchy defined by layers of organizational units (OUs). Security controls can then be organized and applied to the OUs and member accounts, establishing consistent preventative controls on member accounts in the organization.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_securely_operate_multi_accounts.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_12_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_13",
                    "title": "Do you configure IDP services and application logging?",
                    "description": "During a security investigation or other use cases based on your requirements, you need to be able to review relevant logs to record and understand the full scope and timeline of the incident. Logs are also required for alert generation, indicating that certain actions of interest have happened.",
                    "helpfulResource": {
                        "displayText": "Retain security event logs from services and applications. This is a fundamental principle of security for audit, investigations, and operational use cases, and a common security requirement driven by governance, risk, and compliance (GRC) standards, policies, and procedures.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_detect_investigate_events_app_service_logging.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_13_1",
                            "title": "Logging Amazon Textract and Comprehend API calls with AWS CloudTrail",
                            "helpfulResource": {
                                "displayText": "Both Amazon Textract and Amazon Comprehend are integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service. CloudTrail captures API calls for Textract and Comprehend as events. The calls captured include calls from the service console and code calls to the service API operations.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/logging-using-cloudtrail.html"
                            },
                            "improvementPlan": {
                                "displayText": "If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon Textract/Comprehend. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/logging-using-cloudtrail.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Logging Amazon Comprehend API calls with AWS CloudTrail",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/logging-using-cloudtrail.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Logging Amazon Comprehend API calls with AWS CloudTrail",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/logging-using-cloudtrail.html"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_13_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SEC_14",
                    "title": "Do you have incident response procedures?",
                    "description": "Do you have incident response procedures in place to handle potential security incidents in your doc processing system?",
                    "helpfulResource": {
                        "displayText": "Even with extremely mature preventive and detective controls, your organization should still put processes in place to respond to and mitigate the potential impact of security incidents.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-incresp.html"
                    },
                    "choices": [
                        {
                            "id": "SEC_14_1",
                            "title": "Establish incident response procedures",
                            "helpfulResource": {
                                "displayText": "The architecture of your workload strongly affects the ability of your teams to operate effectively during an incident, to isolate or contain systems, and to restore operations to a known good state.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-incresp.html"
                            },
                            "improvementPlan": {
                                "displayText": "Putting in place the tools and access ahead of a security incident, then routinely practicing incident response through game days, will help you verify that your architecture can accommodate timely investigation and recovery.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sec-incresp.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SEC_14_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                }
            ]
        },
        {
            "id": "reliability",
            "name": "Reliability",
            "questions": [
                {
                    "id": "REL_1",
                    "title": "How do you manage service quotas and constraints in your IDP workload?",
                    "description": "Service quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can write/read files to/from Amazon S3.",
                    "helpfulResource": {
                        "displayText": "Service quotas exist to prevent accidentally provisioning more resources than you need and to limit request rates on API operations so as to protect services from abuse. There are also resource constraints, for example, the rate that you can write/read files to/from Amazon S3."
                    },
                    "choices": [
                        {
                            "id": "REL_1_1",
                            "title": "Adjust Amazon Textract and Amazon Comprehend service quota values to meet your use case.",
                            "helpfulResource": {
                                "displayText": "When requesting an increase to a default quota, there are several recommended best practices to follow. These include smooth spiky traffic, configuring retries, and configuring exponential backoff and jitters.\nFor Amazon Textract, estimate your optimal quota values using Textract Service Quota Calculator.As an alternative to raising a request directly from the calculator, you can also use the Service quotas console.\nFor Amazon Comprehend use the Service quotas console.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html"
                            },
                            "improvementPlan": {
                                "displayText": "When requesting an increase to a default quota, there are several recommended best practices to follow. These include smooth spiky traffic, configuring retries, and configuring exponential backoff and jitters.\nFor Amazon Textract, estimate your optimal quota values using Textract Service Quota Calculator.As an alternative to raising a request directly from the calculator, you can also use the Service quotas console.\nFor Amazon Comprehend use the Service quotas console.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend endpoints and quotas",
                                      "url": "https://docs.aws.amazon.com/general/latest/gr/comprehend.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend endpoints and quotas",
                                      "url": "https://docs.aws.amazon.com/general/latest/gr/comprehend.html"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "REL_1_2",
                            "title": "Be aware of unchangeable Amazon Textract and Amazon Comprehend Service Quotas, limits and contraints",
                            "helpfulResource": {
                                "displayText": "When designing your IDP workflow architecture, be aware of Amazon Textract and Amazon Comprehend service limits and quotas, which cannot be changed.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-document.html"
                            },
                            "improvementPlan": {
                                "displayText": "Design your IDP workflow architecture to prevent these limits from impacting reliability.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-document.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend - Guidelines and quotas",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html"
                                    },
                                    {
                                      "displayText": "Amazon Comprehend - Best practices for images",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/idp-images-bp.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Amazon Comprehend - Guidelines and quotas",
                                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/guidelines-and-limits.html"
                                    },
                                    {
                                        "displayText": "Amazon Comprehend - Best practices for images",
                                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/idp-images-bp.html"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "REL_1_3",
                            "title": "Manage service quotas across accounts and regions",
                            "helpfulResource": {
                                "displayText": "If you are using multiple accounts or Regions, request the appropriate quotas in all environments in which your production workloads run.",
                                "url": "https://docs.aws.amazon.com/pdfs/wellarchitected/latest/reliability-pillar/wellarchitected-reliability-pillar.pdf#welcome"
                            },
                            "improvementPlan": {
                                "displayText": "If you are using multiple accounts or Regions, request the appropriate quotas in all environments in which your production workloads run.",
                                "url": "https://docs.aws.amazon.com/pdfs/wellarchitected/latest/reliability-pillar/wellarchitected-reliability-pillar.pdf#welcome"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "REL_1_1 && REL_1_2 && REL_1_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "REL_2",
                    "title": "How do you design your IDP workload to adopt to changes?",
                    "description": "Changes to your workload or its environment must be anticipated and accommodated to achieve reliable operation of the workload. Changes include those imposed on your workload such as spikes in demand, as well as those from within such as feature deployments and security patches.",
                    "helpfulResource": {
                        "displayText": "Changes to your workload or its environment must be anticipated and accommodated to achieve reliable operation of the workload. Changes include those imposed on your workload such as spikes in demand, as well as those from within such as feature deployments and security patches."
                    },
                    "choices": [
                        {
                            "id": "REL_2_1",
                            "title": "Use Amazon Comprehend flywheel to simplify the process of improving a custom model over time.",
                            "helpfulResource": {
                                "displayText": "Use Amazon Comprehend flywheel to simplify the process of improving a custom model over time. You can create a flywheel to use an existing trained model or create and train a new one. Flywheel orchestrate the tasks associated with training and evaluating new custom model versions.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/simplify-continuous-learning-of-amazon-comprehend-custom-models-using-comprehend-flywheel/"
                            },
                            "improvementPlan": {
                                "displayText": "Use Amazon Comprehend flywheel to simplify the process of improving a custom model over time. You can create a flywheel to use an existing trained model or create and train a new one. Flywheel orchestrate the tasks associated with training and evaluating new custom model versions.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/simplify-continuous-learning-of-amazon-comprehend-custom-models-using-comprehend-flywheel"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend Flywheels simplifies the process of improving a custom model over time.",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/flywheels.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Amazon Comprehend Flywheels simplifies the process of improving a custom model over time.",
                                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/flywheels.html"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "REL_2_2",
                            "title": "Monitor, send notifications and automate responses using Amazon CloudWatch",
                            "helpfulResource": {
                                "displayText": "Use Amazon CloudWatch to monitor your IDP workflow component, such as Amazon Textract and Amazon Comprehend. Collect metrics from IDP workflow components, automate response to alarms and send notifications as required to your workload",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/manage-endpoints-monitor.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use Amazon CloudWatch to monitor Amazon Comprehend document classification and entity recognizer endpoints.\nUse auto scaling feature to automatically set endpoint provisioning to fit your capacity needs.\n\nUse Amazon CloudWatch to monitor Amazon Textract metrics, such as request errors, latency or if your application has reached the maximum number of requests per second.\n\nUse Amazon CloudWatch alarms to notify you when one or more metrics fall outside a defined threshold.\n",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/manage-endpoints-monitor.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend - Auto scaling with endpoints",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html"
                                    },
                                    {
                                        "displayText": "Monitoring Amazon Textract",
                                        "url": "https://docs.aws.amazon.com/textract/latest/dg/textract-monitoring.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Amazon Comprehend - Auto scaling with endpoints",
                                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html"
                                      },
                                      {
                                          "displayText": "Monitoring Amazon Textract",
                                          "url": "https://docs.aws.amazon.com/textract/latest/dg/textract-monitoring.html"
                                      }]
                                }
                            ]
                        },
                        {
                            "id": "REL_2_3",
                            "title": "Use IaC, such as CDK and pre-built IDP CDK constructs to deploy IDP workflows",
                            "helpfulResource": {
                                "displayText": "Deploy all changes with automation, using IaC, removes the potential for introduction of human error and provides the ability to test before changing production environment",
                                "url": "https://github.com/aws-samples/amazon-textract-idp-cdk-constructs"
                            },
                            "improvementPlan": {
                                "displayText": "Deploy all changes with automation, using IaC, removes the potential for introduction of human error and provides the ability to test before changing production environment",
                                "url": "https://github.com/aws-samples/amazon-textract-idp-cdk-constructs"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Textract IDP Stack Samples",
                                      "url": "https://github.com/aws-solutions-library-samples/guidance-for-low-code-intelligent-document-processing-on-aws"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Amazon Textract IDP Stack Samples",
                                        "url": "https://github.com/aws-solutions-library-samples/guidance-for-low-code-intelligent-document-processing-on-aws"
                                      }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "REL_2_1 && REL_2_2 && REL_2_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "REL_3",
                    "title": "How do you prepare your IDP workflow to manage and withstand failures?",
                    "description": "In any system of reasonable complexity, it is expected that failures will occur. Reliability requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues.",
                    "helpfulResource": {
                        "displayText": "In any system of reasonable complexity, it is expected that failures will occur. Reliability requires that your workload be aware of failures as they occur and take action to avoid impact on availability. Workloads must be able to both withstand failures and automatically repair issues."
                    },
                    "choices": [
                        {
                            "id": "REL_3_1",
                            "title": "Use Amazon S3 as your scalable datastore for IDP workflow Data capture stage.",
                            "helpfulResource": {
                                "displayText": "Amazon S3 provides a highly durable storage infrastructure designed for mission-critical and primary data storage.",
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html"
                            },
                            "improvementPlan": {
                                "displayText": "Consider Amazon S3 cross-region replication to futher increase the reliability and take advantage of disaster recovery options",
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Protect Data on Amazon S3 Against Accidental Deletion or Application Bugs Using S3 Versioning, S3 Object Lock, and S3 Replication",
                                      "url": "https://aws.amazon.com/getting-started/hands-on/protect-data-on-amazon-s3/?ref=docs_gateway/amazons3/DataDurability.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Protect Data on Amazon S3 Against Accidental Deletion or Application Bugs Using S3 Versioning, S3 Object Lock, and S3 Replication",
                                        "url": "https://aws.amazon.com/getting-started/hands-on/protect-data-on-amazon-s3/?ref=docs_gateway/amazons3/DataDurability.html"
                                      }]
                                }
                            ]
                        },
                        {
                            "id": "REL_3_2",
                            "title": "Identify and back up IDP workflow data that needs to be backed up.",
                            "helpfulResource": {
                                "displayText": "Back up all IDP workflow data according to your business requirements. In case of data lost, the strategy implemented allows recovery or the reproduction of data within the defined RPO and RTO.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_backing_up_data_identified_backups_data.html"
                            },
                            "improvementPlan": {
                                "displayText": "Back up all IDP workflow data according to your business requirements. Analyse the storage for your IDP Data Capture and consumption stages. Then, establish a strategy for data recovery based on the RPO. This strategy involves either backing up these data sources, or having the ability to reproduce data from other sources. In case of data lodd, the strategy implemented allows recovery or the reproduction of data within the defined RPO and RTO.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_backing_up_data_identified_backups_data.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Disaster recovery options in the cloud",
                                      "url": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Disaster recovery options in the cloud",
                                        "url": "https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"
                                      }]
                                }
                            ]
                        },
                        {
                            "id": "REL_3_3",
                            "title": "Design your workload architecture following the IDP workflow",
                            "helpfulResource": {
                                "displayText": "Although the stages in an IDP workflow may vary and be influenced by use case and business requirements, the stages of data capture, document classification, text extraction, content enrichment, review/validation, and consumption are typically parts of an IDP workflow. Processing documents such as tax forms, claims, medical notes, new customer forms, invoices, legal contracts, and more are just a few of the use cases for IDP.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_service_architecture_monolith_soa_microservice.html"
                            },
                            "improvementPlan": {
                                "displayText": "Although the stages in an IDP workflow may vary and be influenced by use case and business requirements, the stages of data capture, document classification, text extraction, content enrichment, review/validation, and consumption are typically parts of an IDP workflow. Processing documents such as tax forms, claims, medical notes, new customer forms, invoices, legal contracts, and more are just a few of the use cases for IDP.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_service_architecture_monolith_soa_microservice.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "AWS Machine Learning Blog: Intelligent document processing with AWS AI services: Part 1",
                                      "url": "https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "AWS Machine Learning Blog: Intelligent document processing with AWS AI services: Part 1",
                                        "url": "https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/"
                                      }]
                                }
                            ]
                        },
                        {
                            "id": "REL_3_4",
                            "title": "Implement loosely coupled IDP workflow stages",
                            "helpfulResource": {
                                "displayText": "Use Amazon SQS to decouple an IDP architecture. Decoupling pattern helps to isolate behavior of architecture components from other components that depend on it, increasing resiliency and agility.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_prevent_interaction_failure_loosely_coupled_system.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use Amazon SQS to decouple an IDP workflow stages",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_prevent_interaction_failure_loosely_coupled_system.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Best Practices for Service Quota Increase Requests",
                                      "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html#best-quota-practices"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Best Practices for Service Quota Increase Requests",
                                        "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html#best-quota-practices"
                                      }]
                                }
                            ]
                        },
                        {
                            "id": "REL_4_3",
                            "title": "Control and limit retry calls",
                            "helpfulResource": {
                                "displayText": "An Amazon Textract operation can fail if you exceed the maximum number of transactions per second (TPS), causing the service to throttle your application, or when your connection drops. For example, if you make too many calls to Amazon Textract operations in a short period of time.\nYou can manage throttling and dropped connections by automatically retrying the operation. You can specify the number of retries by including the Config parameter when you create the Amazon Textract client. We recommend a retry count of 5. The AWS SDK retries an operation the specified number of times before failing and throwing an exception.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/handling-errors.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use exponential backoff to retry requests at progressively longer intervals between each retry. Introduce jitter between retries to randomize retry intervals. Limit the maximum number of retries.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/handling-errors.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Retry behavior",
                                      "url": "https://docs.aws.amazon.com/sdkref/latest/guide/feature-retry-behavior.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "Retry behavior",
                                        "url": "https://docs.aws.amazon.com/sdkref/latest/guide/feature-retry-behavior.html"
                                      }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "REL_3_1 && REL_3_2 && REL_3_3 && REL_3_4 && REL_4_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                }
            ]
        },
        {
            "id": "performance",
            "name": "Performance Efficiency",
            "questions": [
                {
                    "id": "PERF_1",
                    "title": "Does your solution follow the IDP workflow?",
                    "description": "The stages in an IDP workflow may vary and be influenced by use case and business requirements. However in general, an IDP workflow includes some typical stages",
                    "helpfulResource": {
                        "displayText": "The stages in an IDP workflow may vary and be influenced by use case and business requirements. However in general, an IDP workflow includes some typical stages",
                        "url": "https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/"
                    },
                    "choices": [
                        {
                            "id": "PERF_1_1",
                            "title": "The project includes six phases in the workload.",
                            "helpfulResource": {
                                "displayText": "Although the stages in an IDP workflow may vary and be influenced by use case and business requirements, the stages of data capture, document classification, text extraction, content enrichment, review/validation, and consumption are typically parts of an IDP workflow. Processing documents such as tax forms, claims, medical notes, new customer forms, invoices, legal contracts, and more are just a few of the use cases for IDP.",
                                "url": "https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/"
                            },
                            "improvementPlan": {
                                "displayText": "Design multiple stages in IDP workflow",
                                "url": "https://aws.amazon.com/blogs/machine-learning/part-2-intelligent-document-processing-with-aws-ai-services/"
                            }
                        },
                        {
                            "id": "PERF_1_2",
                            "title": "The project includes 3-5 phases in the workload.",
                            "helpfulResource": {
                                "displayText": "Although the stages in an IDP workflow may vary and be influenced by use case and business requirements, the stages of data capture, document classification, text extraction, content enrichment, review/validation, and consumption are typically parts of an IDP workflow. Processing documents such as tax forms, claims, medical notes, new customer forms, invoices, legal contracts, and more are just a few of the use cases for IDP."
                            },
                            "improvementPlan": {
                                "displayText": "Design multiple stages in IDP workflow"
                            }
                        },
                        {
                            "id": "PERF_1_3",
                            "title": "The project includes 0-2 phases in the workload.",
                            "helpfulResource": {
                                "displayText": "Although the stages in an IDP workflow may vary and be influenced by use case and business requirements, the stages of data capture, document classification, text extraction, content enrichment, review/validation, and consumption are typically parts of an IDP workflow. Processing documents such as tax forms, claims, medical notes, new customer forms, invoices, legal contracts, and more are just a few of the use cases for IDP."
                            },
                            "improvementPlan": {
                                "displayText": "Design multiple stages in IDP workflow"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_1_1 && PERF_1_2 && PERF_1_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_2",
                    "title": "How do you decouple the IDP architecture components when document volume is high?",
                    "description": "For instance, you need to process more than 50 documents in Ireland region, which is higher than the maximum number of asynchronous Analyze Lending jobs that can simultaneously exist. When thedocument volume is over thanTextract service quotas, we need to decouple an IDP architecture.",
                    "helpfulResource": {
                        "displayText": "For instance, you need to process more than 50 documents in Ireland region, which is higher than the maximum number of asynchronous Analyze Lending jobs that can simultaneously exist. When thedocument volume is over thanTextract service quotas, we need to decouple an IDP architectureto avoid throttling issue, as well as for better extendability and performance efficiency.",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/async-analyzing-with-sqs.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_2_1",
                            "title": "Did not monitor the user traffic, not clear about the highest TPS for the workload.",
                            "helpfulResource": {
                                "displayText": "You can useAmazon Textract endpoints and quotas to help you evaluate the critial API calls.",
                                "url": "https://docs.aws.amazon.com/general/latest/gr/textract.html"
                            },
                            "improvementPlan": {
                                "displayText": "Decouple an IDP architecture using Amazon SQS, Amazon Textract async API, or leverage Amazon Textract IDP CDK Constructs to do the decoupling implementation ",
                                "url": "https://docs.aws.amazon.com/general/latest/gr/textract.html"
                            }
                        },
                        {
                            "id": "PERF_2_2",
                            "title": "Use Amazon SQS to decouple an IDP architecture",
                            "helpfulResource": {
                                "displayText": "You can use Amazon SQS between service components of your architecture to decouple an architecture. Or you can consider using Amazon Textract IDP CDK Constructs which has built-in SQS to implement an IDP architecture.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/async-analyzing-with-sqs.html, https://catalog.us-east-1.prod.workshops.aws/workshops/f2dd7c46-e022-4f9c-8399-dcad742be516/en-US/lab-3/step-3a"
                            },
                            "improvementPlan": {
                                "displayText": "Decouple an IDP architecture using Amazon SQS, Amazon Textract async API, or leverage Amazon Textract IDP CDK Constructs to do the decoupling implementation ",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/async-analyzing-with-sqs.html, https://catalog.us-east-1.prod.workshops.aws/workshops/f2dd7c46-e022-4f9c-8399-dcad742be516/en-US/lab-3/step-3a"
                            }
                        },
                        {
                            "id": "PERF_2_3",
                            "title": "Leverage async APIs to decouple an IDP architecture",
                            "helpfulResource": {
                                "displayText": "Async APIs can be used to handling event-based processing, long-running tasks, highly concurrent systems, rate limit scenarios, and distributed systems.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/sync-calling.html"
                            },
                            "improvementPlan": {
                                "displayText": "Decouple an IDP architecture using Amazon SQS, Amazon Textract async API, or leverage Amazon Textract IDP CDK Constructs to do the decoupling implementation ",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/async.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_2_1 && PERF_2_2 && PERF_2_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_3",
                    "title": "Use of output config to scale the async workload",
                    "description": "(For Async Workload) Use of output config to scale the async workload.This will mitigate customers hitting throttling on Get requests (GetDocumentAnalysis or GetDocumentTextDetection) as they can directly consume the output from their S3 bucket.",
                    "helpfulResource": {
                        "displayText": "(For Async Workload) Use of output config to scale the async workload.This will mitigate customers hitting throttling on Get requests (GetDocumentAnalysis or GetDocumentTextDetection) as they can directly consume the output from their S3 bucket.",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/API_OutputConfig.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_3_1",
                            "title": "Optimise with Output Config",
                            "helpfulResource": {
                                "displayText": "Use OutputConfig when making Textract API async requests.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/store-output-in-custom-amazon-s3-bucke[]-kms-for-multi-page-document-processing-with-amazon-textract/"
                            },
                            "improvementPlan": {
                                "displayText": "Use OutputConfig when making Textract API async requests."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_3_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_4",
                    "title": "Are the limits for TPS configured correct for the production workload. Start API TPS, job queue limit, token bucket for proces",
                    "description": "Use the TPS calculator in the Textract console to identify the TPS limits needed for your workload. Start APIs include, StartDocumentAnalysis, StartDocumentTextDetection, StartExpenseAnalysis, and StartLendingAnalysis.",
                    "helpfulResource": {
                        "displayText": "TPS refers to the number of Textract API calls (Detect Document Text, Analyze Document, etc.) per second that your workload will need.Depending on the type and complexity of your documents, each API call may process 1 page or multiple pages of a document.",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_4_1",
                            "title": "Have not used TPS calculator to identify the workload limitation.",
                            "helpfulResource": {
                                "displayText": "Have not used TPS calculator to identify the workload limitation.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use TPS Calaulator to indentify the workload limitation",
                                "url": "https://docs.aws.amazon.com/general/latest/gr/textract.html"
                            }
                        },
                        {
                            "id": "PERF_4_2",
                            "title": "Have used TPS calculator to identify the workload limitation.",
                            "helpfulResource": {
                                "displayText": "Have used TPS calculator to identify the workload limitation.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/limits-quotas-explained.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use TPS Calaulator to indentify the workload limitation",
                                "url": "https://docs.aws.amazon.com/general/latest/gr/textract.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_4_1 && PERF_4_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_6",
                    "title": "Are documents processed in parellel or sequential?",
                    "description": "Process documents or pages in parallel, not sequential.\nPre-processing or post-processing steps should be performed in parallel when they dont have dependencies.",
                    "helpfulResource": {
                        "displayText": "Process documents or pages in parallel, not sequential.\nPre-processing or post-processing steps should be performed in parallel when they dont have dependencies.",
                        "url": "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_6_1",
                            "title": "Use a Step Funtion's Parellel state to perform mutiple functions at the same time.",
                            "helpfulResource": {
                                "displayText": "A Parallel state causes AWS Step Functions to execute each branch, starting with the state named in that branch's StartAt field, as concurrently as possible, and wait until all branches terminate (reach a terminal state) before processing the Parallel state's Next field.",
                                "url": "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use a Step Funtion's Parellel state to perform mutiple functions at the same time.",
                                "url": "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-parallel-state.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_6_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_7",
                    "title": "When using Textract Start APIs do the Lambda function wait for a response before continuing?",
                    "description": "If the Start API is call and the Lambda waits for the job to finish, the job could exceed the Lambda timeout. f it exceeds the Lambda timeout, the function will terminate and restart, leading to multiple calls to the Textract service. This will incur additional cost and the execution of the call will not be successful.",
                    "helpfulResource": {
                        "displayText": "If the Start API is call and the Lambda waits for the job to finish, the job could exceed the Lambda timeout. f it exceeds the Lambda timeout, the function will terminate and restart, leading to multiple calls to the Textract service. This will incur additional cost and the execution of the call will not be successful.",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_7_1",
                            "title": "Use queues to handle Textract API responses.",
                            "helpfulResource": {
                                "displayText": "StartDocumentAnalysis returns a job identifier (JobId) that you use to get the results of the operation. When text analysis is finished, Amazon Textract publishes a completion status to the Amazon Simple Notification Service (Amazon SNS) topic that you specify in NotificationChannel. To get the results of the text analysis operation, first check that the status value published to the Amazon SNS topic is SUCCEEDED. If so, call GetDocumentAnalysisand pass the job identifier (JobId) from the initial call to StartDocumentAnalysis.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use queues to handle Textract API responses.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html"
                            }
                        },
                        {
                            "id": "PERF_7_2",
                            "title": "Handling Throttled Calls and Dropped Connections",
                            "helpfulResource": {
                                "displayText": "You can manage throttling and dropped connections by automatically retrying the operation. You can specify the number of retries by including the Config parameter when you create the Amazon Textract client. We recommend a retry count of 5. The AWS SDK retries an operation the specified number of times before failing and throwing an exception.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/handling-errors.html"
                            },
                            "improvementPlan": {
                                "displayText": "Handling Throttled Calls and Dropped Connections",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/handling-errors.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_7_1 && PERF_7_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_8",
                    "title": "What type of storage do you use for the Data capture phase in the IDP workflow?",
                    "description": "IDP workflow starts with a data capture stage to securely store and aggregate different file formats (PDF, JPEG, PNG, TIFF) and layouts of documents from different sources.",
                    "helpfulResource": {
                        "displayText": "IDP workflow starts with a data capture stage to securely store and aggregate different file formats (PDF, JPEG, PNG, TIFF) and layouts of documents from different sources. ",
                        "url": "https://docs.aws.amazon.com/s3/index.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_8_1",
                            "title": "Use S3 to securely store documents for the IDP",
                            "helpfulResource": {
                                "displayText": "S3 is highly scalable and durable storage. itoffers industry-leading scalability, data availability, security, and performance. Amazon S3 is designed for 11 9s of durability.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/"
                            },
                            "improvementPlan": {
                                "displayText": "Use S3 to securely store documents for the IDP",
                                "url": "https://aws.amazon.com/blogs/machine-learning/part-1-intelligent-document-processing-with-aws-ai-services/"
                            }
                        },
                        {
                            "id": "PERF_8_2",
                            "title": "Use DynamoDB to securely store documents for the IDP",
                            "helpfulResource": {
                                "displayText": "Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale.",
                                "url": "https://docs.aws.amazon.com/dynamodb/"
                            },
                            "improvementPlan": {
                                "displayText": "The document metadata, and extracted information can store at DynamoDB.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/real-estate-brokerage-firm-john-l-scott-uses-amazon-textract-and-amazon-comprehend-to-strike-racially-restrictive-language-from-property-deeds-for-homeowners/"
                            }
                        },
                        {
                            "id": "PERF_8_3",
                            "title": "Use Kendra or other Vector Storage to securely store documents for the IDP",
                            "helpfulResource": {
                                "displayText": "Amazon Kendra is an intelligent enterprise search service that helps you search across different content repositories with built-in connectors.",
                                "url": "https://docs.aws.amazon.com/kendra/"
                            },
                            "improvementPlan": {
                                "displayText": "The document metadata, and extracted information can store at Kendra for document retrieval later.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/augment-search-with-metadata-by-chaining-amazon-textract-amazon-comprehend-and-amazon-kendra/"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "AWS Machine Learning Blog : Bring structure to diverse documents with Amazon Textract and transformer-based models on Amazon SageMaker",
                                      "url": "https://aws.amazon.com/blogs/machine-learning/bring-structure-to-diverse-documents-with-amazon-textract-and-transformer-based-models-on-amazon-sagemaker/"
                                    }]
                                }
                            ]                            
                        },
                        {
                            "id": "PERF_8_4",
                            "title": "Others",
                            "helpfulResource": {
                                "displayText": "Other AWS services",
                                "url": "https://docs.aws.amazon.com/"
                            },
                            "improvementPlan": {
                                "displayText": "Dive deep your use case with AWS account SA",
                                "url": "https://docs.aws.amazon.com/"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_8_1 && PERF_8_2 && PERF_8_3 && PERF_8_4",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_9",
                    "title": "How do you get notification and setup retry mechanism when a document fails?",
                    "description": "How do you receive notifications or alerts when document processing fails?  Are there mechanisms in place to notify relevant stakeholders or trigger automated actions based on the failiure events?Do you have defined process (auto/manual) for resolving failed document processing, including the identification of root causes and subsequent corrective actions?\n\nWhat is your approach to handling failed documents? \n\nHave you implemented a retry mechanism to auto retry failed document processing attempts? Does your retry mechanism implement strategies such as exponential backoff/Client Side Rate Limiting to avoid running into TPS limits?",
                    "helpfulResource": {
                        "displayText": "An Amazon Textract operation can fail if you exceed the maximum number of transactions per second (TPS), causing the service to throttle your application, or when your connection drops. For example, if you make too many calls to Amazon Textract operations in a short period of time, it throttles your calls and sends a ProvisionedThroughputExceededException error in the operation response. For information about Amazon Textract TPS quotas, see Amazon Textract Quotas. To change a limit, you can access the Amazon Textract option in the Service Quotas console.\nYou can manage throttling and dropped connections by automatically retrying the operation. You can specify the number of retries by including the Config parameter when you create the Amazon Textract client. We recommend a retry count of 5. The AWS SDK retries an operation the specified number of times before failing and throwing an exception. For more information, see Error Retries and Exponential Backoff in AWS.",
                        "url": "https://docs.aws.amazon.com/sdkref/latest/guide/feature-retry-behavior.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_9_1",
                            "title": "Config the error handing of exceeding the maximum transactions per second (TPS) limit",
                            "helpfulResource": {
                                "displayText": "Config the error handing of exceeding the maximum transactions per second (TPS) limit",
                                "url": "https://docs.aws.amazon.com/general/latest/gr/textract.html"
                            },
                            "improvementPlan": {
                                "displayText": "Config the error handing of exceeding the maximum transactions per second (TPS) limit"
                            }
                        },
                        {
                            "id": "PERF_9_2",
                            "title": "Setup the exponential backoff on AWS.",
                            "helpfulResource": {
                                "displayText": "Setup the exponential backoff on AWS.",
                                "url": "https://docs.aws.amazon.com/sdkref/latest/guide/feature-retry-behavior.html"
                            },
                            "improvementPlan": {
                                "displayText": "Setup the exponential backoff on AWS.",
                                "url": "https://docs.aws.amazon.com/sdkref/latest/guide/feature-retry-behavior.html"
                            }
                        },
                        {
                            "id": "PERF_9_3",
                            "title": "Setup a retry workflow and follow up mechanisim to solve the unfinished workflow.",
                            "helpfulResource": {
                                "displayText": "Setup a retry workflow and follow up mechanisim to solve the unfinished workflow."
                            },
                            "improvementPlan": {
                                "displayText": "Setup a retry workflow and follow up mechanisim to solve the unfinished workflow."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_9_1 && PERF_9_2 && PERF_9_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_10",
                    "title": "Use OutputConfig and SNS notifications to avoid calling Get for all the paginated responses and avoid running into TPS limits ",
                    "description": "Are you leveraging Textract's output configuration & SNS options to be notified of job completion to retrieve the required data without having to make redundant/repetitive calls?\n\nHave you configured Textract to send SNS notifications upon job completion instead of continously polling for job statuses?",
                    "helpfulResource": {
                        "displayText": "Are you leveraging Textract's output configuration & SNS options to be notified of job completion to retrieve the required data without having to make redundant/repetitive calls?\n\nHave you configured Textract to send SNS notifications upon job completion instead of continously polling for job statuses?",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/api-async-roles.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_10_1",
                            "title": "Retrywithout having to make redundant/repetitive calls",
                            "helpfulResource": {
                                "displayText": "Retrywithout having to make redundant/repetitive calls"
                            },
                            "improvementPlan": {
                                "displayText": "Retrywithout having to make redundant/repetitive calls"
                            }
                        },
                        {
                            "id": "PERF_10_2",
                            "title": "Textract to send SNS notifications upon job completion instead of continously polling for job statuses.",
                            "helpfulResource": {
                                "displayText": "Textract to send SNS notifications upon job completion instead of continously polling for job statuses."
                            },
                            "improvementPlan": {
                                "displayText": "Textract to send SNS notifications upon job completion instead of continously polling for job statuses."
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_10_1 && PERF_10_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_11",
                    "title": "How often do you compare the performance of Comprehend Custom Classifier after rolling out the service on production?",
                    "description": "Amazon Comprehend provides you with metrics based on training of the recognizer model. These metrics provide an insight into how accurately the trained model will perform when you use it to identify entities. ",
                    "helpfulResource": {
                        "displayText": "Amazon Comprehend provides you with metrics based on training of the recognizer model. These metrics provide an insight into how accurately the trained model will perform when you use it to identify entities. ",
                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/flywheels.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_11_1",
                            "title": "Keep track and evaluatethe performance of Comprehend Custom Classifier model",
                            "helpfulResource": {
                                "displayText": "Have in place a process to monitor and improve a custom model over time",
                                "url": "https://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-comprehend-flywheel-for-mlops/"
                            },
                            "improvementPlan": {
                                "displayText": "Have in place a process to monitor and improve a custom model over time",
                                "url": "https://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-comprehend-flywheel-for-mlops/"
                            }
                        },
                        {
                            "id": "PERF_11_2",
                            "title": "Establish data quality assurance process to prepare data for training",
                            "helpfulResource": {
                                "displayText": "Make sure to follow the guidelines in the respective documentation to improve data quality.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html"
                            },
                            "improvementPlan": {
                                "displayText": "Establish data quality assurance process to prepare data for training",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "AWS Machine Learning Blog: Build a custom entity recognizer for PDF documents using Amazon Comprehend",
                                      "url": "https://aws.amazon.com/blogs/machine-learning/build-a-custom-entity-recognizer-for-pdf-documents-using-amazon-comprehend/"
                                    },
                                    {
                                      "displayText": "AWS Machine Learning Blog: Amazon Comprehend announces lower annotation limits for custom entity recognition",
                                      "url": "https://aws.amazon.com/blogs/machine-learning/amazon-comprehend-announces-lower-annotation-limits-for-custom-entity-recognition/"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                        "displayText": "AWS Machine Learning Blog: Build a custom entity recognizer for PDF documents using Amazon Comprehend",
                                        "url": "https://aws.amazon.com/blogs/machine-learning/build-a-custom-entity-recognizer-for-pdf-documents-using-amazon-comprehend/"
                                      },
                                      {
                                        "displayText": "AWS Machine Learning Blog: Amazon Comprehend announces lower annotation limits for custom entity recognition",
                                        "url": "https://aws.amazon.com/blogs/machine-learning/amazon-comprehend-announces-lower-annotation-limits-for-custom-entity-recognition/"
                                      }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_11_1 && PERF_11_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_12",
                    "title": "How do you monitor the ML Performance?",
                    "description": "How often do you monitor, evaluate, and improve the model performance after rolling out the workload on production?Do you use and tools and mechanisim to monitor, evaluate, and improve the model performance?",
                    "helpfulResource": {
                        "displayText": "Do you use and tools and mechanisim to monitor, evaluate, and improve the model performance?",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/model-evaluation.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_12_1",
                            "title": "Don't have the model monitoring mechanisms setup.",
                            "helpfulResource": {
                                "displayText": "Setup system monitoring with tools and mechanisms"
                            },
                            "improvementPlan": {
                                "displayText": "Setup system monitoring with tools and mechanisms"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_12_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_13",
                    "title": "How often do you compare the Textract confidence score performance / WER after rolling out the service on production?",
                    "description": "Textract confidence score performance/WER variantes based on the quality and type of input document. you should monitor Textract performance scores, and raise alarms when thresholds are breached.",
                    "helpfulResource": {
                        "displayText": "Textract confidence score performance/WER variantes based on the quality and type of input document. you should monitor Textract performance scores, and raise alarms when thresholds are breached.",
                        "url": "https://repost.aws/questions/QU338kk0teQJmB6qWR8OmKvg/aws-textract-accuracy-calculation"
                    },
                    "choices": [
                        {
                            "id": "PERF_13_1",
                            "title": "Establish requirements for your input documents",
                            "helpfulResource": {
                                "displayText": "Ensure that input documents follow Amazon Textract best practices to get the best results from your documents",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/textract-best-practices.html"
                            },
                            "improvementPlan": {
                                "displayText": "Establish requirements for your input documents",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/textract-best-practices.html"
                            }
                        },
                        {
                            "id": "PERF_13_2",
                            "title": "Use human review for results that have a low confidence score",
                            "helpfulResource": {
                                "displayText": "In applications sensitive to error detection (false positives), enforce a minimum confidence score threshold. The application should discard results below that threshold or flag situations as requiring a higher level of human scrutiny.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/a2i-textract.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use human review for results that have a low confidence score",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/a2i-textract.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_13_1 && PERF_13_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_14",
                    "title": "Have your company setup a mechanisim to monitor, evaluate, and improve the end-to-end automation rate after rolling out the wor",
                    "description": "Have your company setup a mechanisim to monitor, evaluate, and improve the end-to-end automation rate after rolling out the workload on production? Which can include, metrics monitoring, evaluating bottlenecks, A/B testing, user feedback, incremental enhancement, log analysis, change management, and more.",
                    "helpfulResource": {
                        "displayText": "Have your company setup a mechanisim to monitor, evaluate, and improve the end-to-end automation rate after rolling out the workload on production? Which can include, metrics monitoring, evaluating bottlenecks, A/B testing, user feedback, incremental enhancement, log analysis, change management, and more.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/performance-efficiency-pillar-best-practices-5.html"
                    },
                    "choices": [
                        {
                            "id": "PERF_14_1",
                            "title": "Evaluate data drift",
                            "helpfulResource": {
                                "displayText": "Understand the effects of data drift on model performance. In cases where the data has drifted, the model could generate inaccurate predictions. Consider a strategy that monitors and adapts to data drift through re-training.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/performance-efficiency-pillar-best-practices-5.html"
                            },
                            "improvementPlan": {
                                "displayText": "Evaluate data drift",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/performance-efficiency-pillar-best-practices-5.html"
                            }
                        },
                        {
                            "id": "PERF_14_2",
                            "title": "Include human-in-the-loop monitoring",
                            "helpfulResource": {
                                "displayText": "Use human-in-the-loop monitoring to monitor model performance efficiently. When automating decision processes, the human labeling of model results is a reliable quality test for model inferences.Compare human labels with model inferences to estimate model performance degradation. Perform mitigation as model re-training.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/performance-efficiency-pillar-best-practices-5.html"
                            },
                            "improvementPlan": {
                                "displayText": "Include human-in-the-loop monitoring",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/performance-efficiency-pillar-best-practices-5.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_14_1 && PERF_14_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "PERF_15",
                    "title": "Whatmetrics do youmonitor the IDP workload ?",
                    "description": "Metrics provides a quantifiable way to understand the project outcome. After the IDP workload deployed to production, there are both business metrics and technical metrics can appply to it. For instances, mins of document classification, percentage of automated document extraction, price per page, accuracy per entity extraction, and accuracy per document classfication.",
                    "helpfulResource": {
                        "displayText": "Metrics provides a quantifiable way to understand the project outcome. After the IDP workload deployed to production, there are both business metrics and technical metrics can appply to it. For instances, mins of document classification, percentage of automated document extraction, price per page, accuracy per entity extraction, and accuracy per document classfication."
                    },
                    "choices": [
                        {
                            "id": "PERF_15_1",
                            "title": "Technical Metrics",
                            "helpfulResource": {
                                "displayText": "Technical Metrics: include (1) OCR metrics: WER (2) document classification metrics: confusion metrix (3) entity recognition:Precision,Recall, F1 (4) endpoint latency per each service API (5) invoke endpoint error"
                            },
                            "improvementPlan": {
                                "displayText": "(1) Adjust current technical metrics with the common IDP technical metrics list\n(2) Add Business metrics from common IDP business metrics list to the daily, monthly review metrics"
                            }
                        },
                        {
                            "id": "PERF_15_2",
                            "title": "Business Metrics",
                            "helpfulResource": {
                                "displayText": "Business Metrics: include (1) End-to-EndDocument Processing Time (2) percentage of automated document (3) Time to value when new document type is added"
                            },
                            "improvementPlan": {
                                "displayText": "(1) Adjust current technical metrics with the common IDP technical metrics list\n(2) Add Business metrics from common IDP business metrics list to the daily, monthly review metrics"
                            }
                        },
                        {
                            "id": "PERF_15_3",
                            "title": "Both Technical and Business Metrics",
                            "helpfulResource": {
                                "displayText": "Technical Metrics: include (1) OCR metrics: WER (2) document classification metrics: confusion metrix (3) entity recognition:Precision,Recall, F1 (4) endpoint latency per each service API (5) invoke endpoint error. AndBusiness Metrics: include (1) End-to-End Document Processing Time (2) percentage of automated document (3) Time to value when new document type is added."
                            },
                            "improvementPlan": {
                                "displayText": "(1) Adjust current technical metrics with the common IDP technical metrics listto the daily, monthly review metrics\n(2)Adjust current business metrics with the common IDP business metrics list to the daily, monthly review metrics"
                            }
                        },
                        {
                            "id": "PERF_15_4",
                            "title": "None",
                            "helpfulResource": {
                                "displayText": "No Metric is used at the moment."
                            },
                            "improvementPlan": {
                                "displayText": "(1) Add Technical metrics from common IDP technical metrics list to the daily, monthly review metrics\n(2) Add Business metrics from common IDP business metrics list to the daily, monthly review metrics"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "PERF_15_1 && PERF_15_2 && PERF_15_3 && PERF_15_4",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                }
            ]
        },
        {
            "id": "costOptimization",
            "name": "Cost Optimization",
            "questions": [
                {
                    "id": "COST_1",
                    "title": "Do you establish a cost optimization function or team for the IDP solution?",
                    "description": "Create a team (Cloud Business Office or Cloud Center of Excellence) that is responsible for establishing and maintaining cost awareness across your organization. The team requires people from finance, technology, and business roles across the organization.",
                    "helpfulResource": {
                        "displayText": "Establish a Cloud Business Office (CBO) or Cloud Center of Excellence (CCOE) team that is responsible for establishing and maintaining a culture of cost awareness in cloud computing. It can be an existing individual, a team within your organization, or a new team of key finance, technology and organization stakeholders from across the organization.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_function.html"
                    },
                    "choices": [
                        {
                            "id": "COST_1_1",
                            "title": "We don't have a specific team for cloud business(cost optimization team).",
                            "helpfulResource": {
                                "displayText": "Establishing a team that can take responsibility for cost optimization is critical for successfully implementing cloud technology at scale for your organization.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_function.html"
                            },
                            "improvementPlan": {
                                "displayText": "Define key members: You need to ensure that all relevant parts of your organization contribute and have a stake in cost management. Common teams within organizations typically include: finance, application or product owners, management, and technical teams (DevOps).\n\nDefine goals and metrics: The function needs to deliver value to the organization in different ways. These goals are defined and continually evolve as the organization evolves. Common activities include: creating and running education programs on cost optimization across the organization, developing organization-wide standards, such as monitoring and reporting for cost optimization, and setting workload goals on optimization. This function also needs to regularly report to the organization on the organization's cost optimization capability.\n\nEstablish regular cadence: The group (finance, technology, and business teams) should come together regularly to review their goals and metrics. A typical cadence involves reviewing the state of the organization, reviewing any programs currently running, and reviewing overall financial and optimization metrics. Then key workloads are reported on in greater detail.\n ",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_function.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "AWS Cloud Enterprise Strategy Blog: Creating the Cloud Business Office",
                                      "url": "https://aws.amazon.com/blogs/enterprise-strategy/creating-the-cloud-business-office/"
                                    },
                                    {
                                      "displayText": "Create a Cloud Center of Excellence",
                                      "url": "https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-laying-the-foundation/cloud-center-of-excellence.html"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "COST_1_2",
                            "title": "We have a specific team for cloud business(cost optimization team).",
                            "helpfulResource": {
                                "displayText": "Establishing a team that can take responsibility for cost optimization is critical for successfully implementing cloud technology at scale for your organization.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_function.html"
                            },
                            "improvementPlan": {
                                "displayText": "Define key members: You need to ensure that all relevant parts of your organization contribute and have a stake in cost management. Common teams within organizations typically include: finance, application or product owners, management, and technical teams (DevOps).\n\nDefine goals and metrics: The function needs to deliver value to the organization in different ways. These goals are defined and continually evolve as the organization evolves. Common activities include: creating and running education programs on cost optimization across the organization, developing organization-wide standards, such as monitoring and reporting for cost optimization, and setting workload goals on optimization. This function also needs to regularly report to the organization on the organization's cost optimization capability.\n\nEstablish regular cadence: The group (finance, technology, and business teams) should come together regularly to review their goals and metrics. A typical cadence involves reviewing the state of the organization, reviewing any programs currently running, and reviewing overall financial and optimization metrics. Then key workloads are reported on in greater detail.\n ",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_function.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_1_1 && COST_1_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_1_1) || (!COST_1_2)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_2",
                    "title": "Do you establish a partnership between finance and technology through your development lifecycle?",
                    "description": "Involve finance and technology teams in cost and usage discussions at all stages of your cloud journey. Teams regularly meet and discuss topics such as organizational goals and targets, current state of cost and usage, and financial and accounting practices",
                    "helpfulResource": {
                        "displayText": "Technology teams innovate faster in the cloud due to shortened approval, procurement, and infrastructure deployment cycles.With the adoption of cloud, infrastructure procurement and consumption are no longer beholden to a chain of dependencies. In the cloud model, technology and product teams are no longer just builders, but operators and owners of their products, responsible for most of the activities historically associated with finance and operations teams, including procurement and deployment.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_partnership.html"
                    },
                    "choices": [
                        {
                            "id": "COST_2_1",
                            "title": "We don't involve finance team and goals in our development lifecycle.",
                            "helpfulResource": {
                                "displayText": "Establish a partnership between key finance and technology stakeholders to create a shared understanding of organizational goals and develop mechanisms to succeed financially in the variable spend model of cloud computing. Relevant teams within your organization must be involved in cost and usage discussions at all stages of your cloud journey",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_partnership.html"
                            },
                            "improvementPlan": {
                                "displayText": "Relevant teams within your organization must be involved in cost and usage discussions at all stages of your cloud journey, including:\n\nFinancial leads: CFOs, financial controllers, financial planners, business analysts, procurement, sourcing, and accounts payable must understand the cloud model of consumption, purchasing options, and the monthly invoicing process. Finance needs to partner with technology teams to create and socialize an IT value story, helping business teams understand how technology spend is linked to business outcomes. This way, technology expenditures are viewed not as costs, but rather as investments. Due to the fundamental differences between the cloud (such as the rate of change in usage, pay as you go pricing, tiered pricing, pricing models, and detailed billing and usage information) compared to on-premises operation, it is essential that the finance organization understands how cloud usage can impact business aspects including procurement processes, incentive tracking, cost allocation and financial statements.\n\nTechnology leads: Technology leads (including product and application owners) must be aware of the financial requirements (for example, budget constraints) as well as business requirements (for example, service level agreements). This allows the workload to be implemented to achieve the desired goals of the organization.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_partnership.html"
                            }
                        },
                        {
                            "id": "COST_2_2",
                            "title": "We involve finance team and goals in our development lifecycle.",
                            "helpfulResource": {
                                "displayText": "Establish a partnership between key finance and technology stakeholders to create a shared understanding of organizational goals and develop mechanisms to succeed financially in the variable spend model of cloud computing. Relevant teams within your organization must be involved in cost and usage discussions at all stages of your cloud journey",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_partnership.html"
                            },
                            "improvementPlan": {
                                "displayText": "Relevant teams within your organization must be involved in cost and usage discussions at all stages of your cloud journey, including:\n\nFinancial leads: CFOs, financial controllers, financial planners, business analysts, procurement, sourcing, and accounts payable must understand the cloud model of consumption, purchasing options, and the monthly invoicing process. Finance needs to partner with technology teams to create and socialize an IT value story, helping business teams understand how technology spend is linked to business outcomes. This way, technology expenditures are viewed not as costs, but rather as investments. Due to the fundamental differences between the cloud (such as the rate of change in usage, pay as you go pricing, tiered pricing, pricing models, and detailed billing and usage information) compared to on-premises operation, it is essential that the finance organization understands how cloud usage can impact business aspects including procurement processes, incentive tracking, cost allocation and financial statements.\n\nTechnology leads: Technology leads (including product and application owners) must be aware of the financial requirements (for example, budget constraints) as well as business requirements (for example, service level agreements). This allows the workload to be implemented to achieve the desired goals of the organization.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_cloud_financial_management_partnership.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_2_1 && COST_2_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_2_1) || (!COST_2_2)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_3",
                    "title": "How do you manage Amazon Comprehend endpoint's inference units (IUs) ?",
                    "description": "In Amazon Comprehend, endpoints make your custom models available for real-time classification or entity detection. After you create an endpoint, you can make changes to it as your business needs evolve. For example, you can monitor your endpoint utilization and apply auto scaling to automatically set endpoint provisioning to fit your capacity needs.",
                    "helpfulResource": {
                        "displayText": "Amazon Comprehend assigns throughput to an endpoint using Inference units (IU). An IU represents data throughput of 100 characters per second. You can provision the endpoint with up to 10 inference units. You can scale the endpoint throughput either up or down by updating the endpoint.",
                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/using-endpoints.html"
                    },
                    "choices": [
                        {
                            "id": "COST_3_1",
                            "title": "After creating an endpoint, we monitor it with Amazon CloudWatch, update it to change its inference units, or delete it when no",
                            "helpfulResource": {
                                "displayText": "Depending on your needs, you might need to adjust the throughput of your endpoint after creating it. This can be achieved by updating the endpoint's inference units (IUs). When you edit an endpoint, you can add more IUs to an endpoint, or you can decrease the IUs.Based on the CloudWatch metrics, you can also set up auto scaling to automatically adjust the throughput of your endpoint. ",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/manage-endpoints.html"
                            },
                            "improvementPlan": {
                                "displayText": "\nIf you are not actively using the endpoint for an extended period, you should set up an auto scaling policy to reduce your costs.\n\nIf you are no longer using an endpoint you can delete the endpoint to avoid incurring additional cost.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/manage-endpoints.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_3_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_3_1)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_4",
                    "title": "How do you chooseasynchronous inference vs synchronous inference for Comprehend custom classifier or custom entity recognizer i",
                    "description": "Given the considerable cost difference between asynchronous and synchronous inference choices for Amazon Comprehend custom models, what are some key factors you take into account when making the choice to balance between business needs and cost optimization?",
                    "helpfulResource": {
                        "displayText": "Consider your inference need with Amazon Comprehend custom models: do you need inference responses in real-time? Do you need to process multiple documents in one batch? How big are the documents you need to process?",
                        "url": "https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html"
                    },
                    "choices": [
                        {
                            "id": "COST_4_1",
                            "title": "We decide based on if we need real-time response and document size.",
                            "helpfulResource": {
                                "displayText": "Adopt synchronous inference for real-time processing of a single document; chooseasynchronous jobs to analyze large documents or multiple documents in one batch",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/running-class-sync.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use asynchronous jobs to process your multiple documents as a batch, or a single document that is very large (see URL for input data size limits).\n\nAdopt synchronous inference endpoint only when you need to process a single document (see URL for input data size limits) in real-time.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/idp-inputs-async.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend - Running asynchronous jobs",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/running-classifiers.html"
                                    }]
                                },
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Amazon Comprehend - Running asynchronous jobs",
                                      "url": "https://docs.aws.amazon.com/comprehend/latest/dg/running-classifiers.html"
                                    }]
                                }
                            ]
                        }                        
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_4_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_4_1)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_5",
                    "title": "Do you run your experiment workload in a separate account to take advantage of AWS Free Tier?",
                    "description": "The AWS Free Tier provides customers the ability to explore and try out AWS services free of charge up to specified limits for each service. ",
                    "helpfulResource": {
                        "displayText": "The AWS Free Tier provides new AWS customers with free usage tiers for certain AWS services to help you get started. If you have a new idea that youd like to launch or if you have an existing experiment you want to run in the cloud, this is a great way to get started for free.",
                        "url": "https://aws.amazon.com/free/"
                    },
                    "choices": [
                        {
                            "id": "COST_5_1",
                            "title": "We use AWS Free Tier for experiment workload.",
                            "helpfulResource": {
                                "displayText": "The AWS Free Tier is available to all types of customers  students, entrepreneurs, small businesses, and Fortune 500 companies are all welcome to sign up. If you are linked to an Organization (under AWS Organizations), only one account within the organization can benefit from the Free Tier offers. ",
                                "url": "https://aws.amazon.com/free/free-tier-faqs/"
                            },
                            "improvementPlan": {
                                "displayText": "The AWS Free Tier is comprised of three different types of offerings, a 12-month Free Tier, an Always Free offer, and short term trials. Many of the AI services, such as Textract, Comprehend, have specific free tier specifications. Take advantage of the feature for experimentation workload can help with cost optimization.",
                                "url": "https://docs.aws.amazon.com/whitepapers/latest/how-aws-pricing-works/get-started-with-the-aws-free-tier.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_5_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_6",
                    "title": "Do you have strategies for efficient data storage & management for your IDP workflow?",
                    "description": "What strategies have you implemented to ensure cost-effective storage and management of your data?",
                    "helpfulResource": {
                        "displayText": "Given the main storage service in IDP is Amazon S3, what are some features you can use for cost-effective data storage and management?",
                        "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html"
                    },
                    "choices": [
                        {
                            "id": "COST_6_1",
                            "title": "We adopt Intelligent-Tiering for our data storage in S3.",
                            "helpfulResource": {
                                "displayText": "The S3 Intelligent-Tiering storage class is designed to optimize storage costs by automatically moving data to the most cost-effective access tier when access patterns change, without operational overhead or impact on performance.",
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering.html"
                            },
                            "improvementPlan": {
                                "displayText": "There are two ways to move data into S3 Intelligent-Tiering: (1) Directly PUT data into S3 Intelligent-Tiering by specifying INTELLIGENT_TIERING in the x-amz-storage-class header; (2) Configure S3 Lifecycle configurations to transition objects from S3 Standard or S3 Standard-Infrequent Access to S3 Intelligent-Tiering.\n\nYou can optionally enable S3 Intelligent-Tiering Archive Access and Deep Archive Access tiers on top of the automatically provisioned tiers (Frequent Access tier, Infrequent Access tier, and Archive Instant Access tier) to get the lowest storage cost on data.",
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Using S3 Intelligent-Tiering",
                                      "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-intelligent-tiering.html"
                                    },
                                    {
                                      "displayText": "Managing S3 Intelligent-Tiering",
                                      "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-managing.html"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_6_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_7",
                    "title": "Do you split the document into single pages to process rather than apply all FeatureTypes to the whole multi-page document?",
                    "description": "To analyze a bunch of multi-page documents all having same structure, do you split the document into single pages to process using different FeatureTypes for each page rather than apply all FeatureTypes to the whole document?",
                    "helpfulResource": {
                        "displayText": "FeatureType is a parameter for the Document Analysis (both sync and async) API calls in Textract, currently it includes the following values:TABLES | FORMS | QUERIES | SIGNATURES | LAYOUT.",
                        "url": "https://docs.aws.amazon.com/textract/latest/dg/API_AnalyzeDocument.html"
                    },
                    "choices": [
                        {
                            "id": "COST_7_1",
                            "title": "We split the multi-page documents into single pages to only apply specific FeatureType processing that are absolutely necessary",
                            "helpfulResource": {
                                "displayText": "Amazon Textract charges you based on the number of pages and images processed. Not all pages might include the information you need to extract. Splitting documents into single pages and only focus on the single pages with the FeatureType you need to extract can reduce the cost.",
                                "url": "https://aws.amazon.com/textract/pricing/"
                            },
                            "improvementPlan": {
                                "displayText": "Prepare the documents by splitting them into single pages, perform Document Analysis on the pages that include the FeatureType you need to extract. Specify FeatureType in the Textract API call.",
                                "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-extract-content-from-pdf-files-using-amazon-textract.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon Textract - API Reference: StartDocumentAnalysis",
                                      "url": "https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html#Textract-StartDocumentAnalysis-request-FeatureTypes"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_7_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_8",
                    "title": "Do you enforce data retention policies throughout the IDP workflow?",
                    "description": "Define data retention policies on supported resources to handle object deletion per your organizations requirements. Identify and delete unnecessary or orphaned resources and objects that are no longer required.",
                    "helpfulResource": {
                        "displayText": "Use data retention policies and lifecycle policies to reduce the associated costs of the decommissioning process and storage costs for the identified resources.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_decomissioning_resources_data_retention.html"
                    },
                    "choices": [
                        {
                            "id": "COST_8_1",
                            "title": "We don't enforce data retention policies.",
                            "helpfulResource": {
                                "displayText": "Defining your data retention policies and lifecycle policies to perform automated storage class migration and deletion will reduce the overall storage costs during its lifetime. ",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_decomissioning_resources_data_retention.html"
                            },
                            "improvementPlan": {
                                "displayText": "Set up lifecycle configuration on a bucket for you IDP solution: Use Amazon S3 lifecycle configuration on a bucket to define actions for Amazon S3 to take during an object's lifecycle, as well as deletion at the end of the object's lifecycle, based on your business requirements.",
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon S3 - Managing your storage lifecycle",
                                      "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "COST_8_2",
                            "title": "We enforce data retention policies.",
                            "helpfulResource": {
                                "displayText": "Defining your data retention policies and lifecycle policies to perform automated storage class migration and deletion will reduce the overall storage costs during its lifetime.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_decomissioning_resources_data_retention.html"
                            },
                            "improvementPlan": {
                                "displayText": "Set up lifecycle configuration on a bucket for you IDP solution: Use Amazon S3 lifecycle configuration on a bucket to define actions for Amazon S3 to take during an object's lifecycle, as well as deletion at the end of the object's lifecycle, based on your business requirements.",
                                "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-set-lifecycle-configuration-intro.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "HELPFUL_RESOURCE",
                                  "content": [
                                    {
                                      "displayText": "Amazon S3 - Managing your storage lifecycle",
                                      "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_8_1 && COST_8_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_9",
                    "title": "Do you track resources over their lifetime in the IDP workflow?",
                    "description": "After you manage a list of projects, employees, and technology resources over time you will be able to identify which resources are no longer being used, and which projects that no longer have an owner.",
                    "helpfulResource": {
                        "displayText": "Define and implement a method to track resources and their associations with systems over their lifetime. You can use tagging to identify the workload or function of the resource.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_decomissioning_resources_track.html"
                    },
                    "choices": [
                        {
                            "id": "COST_9_1",
                            "title": "We don't track resources over their lifetime.",
                            "helpfulResource": {
                                "displayText": "Decommission workload resources that are no longer required. Using tags is an effective way to track resources, by labeling the resource with its function, or a known date when it can be decommissioned.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_decomissioning_resources_track.html"
                            },
                            "improvementPlan": {
                                "displayText": "Implement a tagging scheme: Implement a tagging scheme that identifies the workload the resource belongs to, verifying that all resources within the workload are tagged accordingly. Tagging helps you categorize resources by purpose, team, environment, or other criteria relevant to your business. For more detail on tagging uses cases, strategies, and techniques, see AWS Tagging Best Practices.\nImplement workload throughput or output monitoring: Implement workload throughput monitoring or alarming, initiating on either input requests or output completions. Configure it to provide notifications when workload requests or outputs drop to zero, indicating the workload resources are no longer used. Incorporate a time factor if the workload periodically drops to zero under normal conditions. For more detail on unused or underutilized resources, see AWS Trusted Advisor Cost Optimization checks.\nGroup AWS resources: Create groups for AWS resources. You can use AWS Resource Groups to organize and manage your AWS resources that are in the same AWS Region. You can add tags to most of your resources to help identify and sort your resources within your organization. Use Tag Editor add tags to supported resources in bulk. Consider using AWS Service Catalog to create, manage, and distribute portfolios of approved products to end users and manage the product lifecycle.",
                                "url": "https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Use AWS Auto Scaling",
                                      "url": "https://aws.amazon.com/autoscaling/"
                                    },
                                    {
                                        "displayText": "Amazon Comprehend Underutilized Endpoints",
                                        "url": "https://docs.aws.amazon.com/awssupport/latest/user/cost-optimization-checks.html#amazon-comprehend-underutilized-endpoints"
                                    },
                                    {
                                      "displayText": "How do I optimize costs using AWS Trusted Advisor?",
                                      "url": "https://repost.aws/knowledge-center/trusted-advisor-cost-optimization"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "COST_9_2",
                            "title": "We enforce the policy of tracking resources over their lifetime.",
                            "helpfulResource": {
                                "displayText": "Decommission workload resources that are no longer required. Using tags is an effective way to track resources, by labeling the resource with its function, or a known date when it can be decommissioned.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_decomissioning_resources_track.html"
                            },
                            "improvementPlan": {
                                "displayText": "Implement a tagging scheme: Implement a tagging scheme that identifies the workload the resource belongs to, verifying that all resources within the workload are tagged accordingly. Tagging helps you categorize resources by purpose, team, environment, or other criteria relevant to your business. For more detail on tagging uses cases, strategies, and techniques, see AWS Tagging Best Practices.\nImplement workload throughput or output monitoring: Implement workload throughput monitoring or alarming, initiating on either input requests or output completions. Configure it to provide notifications when workload requests or outputs drop to zero, indicating the workload resources are no longer used. Incorporate a time factor if the workload periodically drops to zero under normal conditions. For more detail on unused or underutilized resources, see AWS Trusted Advisor Cost Optimization checks.\nGroup AWS resources: Create groups for AWS resources. You can use AWS Resource Groups to organize and manage your AWS resources that are in the same AWS Region. You can add tags to most of your resources to help identify and sort your resources within your organization. Use Tag Editor add tags to supported resources in bulk. Consider using AWS Service Catalog to create, manage, and distribute portfolios of approved products to end users and manage the product lifecycle.",
                                "url": "https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_9_1 && COST_9_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_9_1) || (!COST_9_2)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_10",
                    "title": "How do you monitor and control cost by AWS services in the IDP solution?",
                    "description": "How do you monitor & control costs associated with the usages of Textract, Comprehend and other AWS IDP suite of services?",
                    "helpfulResource": {
                        "displayText": "What are the cost monitor and control tools you can utilize in line with your organization policies to manage and optimize cloud spend?",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_config_tools.html"
                    },
                    "choices": [
                        {
                            "id": "COST_10_1",
                            "title": "We monitor and control AWS service usage with tools such as AWS Cost Explorer and AWS Budgets and grant team-based access for t",
                            "helpfulResource": {
                                "displayText": "You can use AWS tools like AWS Cost Explorer, AWS Billing, or AWS Budgets for essentials, or you can integrate CUR data with Amazon Athena and Amazon QuickSight to provide this capability for more detailed views. If you don't have essential skills or bandwidth in your organization, you can work with AWS ProServ, AWS Managed Services (AMS), or AWS Partners and use their tools. You can also use third-party tools, but verify first that the cost provides value to your organization.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_detailed_source.html"
                            },
                            "improvementPlan": {
                                "displayText": "Allow team-based access to tools: Configure your accounts and create groups that have access to the required cost and usage reports for their consumptions and use AWS Identity and Access Management to control access to the tools such as AWS Cost Explorer. These groups must include representatives from all teams that own or manage an application. This certifies that every team has access to their cost and usage information to track their consumption.\nConfigure AWS Budgets: Configure AWS Budgets on all accounts for your workload. Set budgets for the overall account spend, and budgets for the workloads by using tags. Configure notifications in AWS Budgets to receive alerts for when you exceed your budgeted amounts, or when your estimated costs exceed your budgets.\nConfigure AWS Cost Explorer: Configure AWS Cost Explorer for your workload and accounts to visualize your cost data for further analysis. Create a dashboard for the workload that tracks overall spend, key usage metrics for the workload, and forecast of future costs based on your historical cost data.\nConfigure AWS Cost Anomaly Detection: Use AWS Cost Anomaly Detection for your accounts, core services, or Cost Categories you created to monitor your cost and usage and detect unusual spends. You can receive alerts individually in aggregated reports, and receive alerts in an email or an Amazon Simple Notification Service topic which allows you to analyze and determine the root cause of the anomaly, and identify the factor that is driving the cost increase.\nConfigure advanced tools: Optionally, you can create custom tools for your organization that provide additional detail and granularity. You can implement advanced analysis capability using Amazon Athena, and dashboards using Amazon QuickSight. Consider using Cloud Intelligence Dashboards (CID) for pre-configured, advanced dashboards. There are also AWS Partners you can work with and adopt their cloud management solutions to activate cloud bill monitoring and optimization in one convenient location.",
                                "url": "https://docs.aws.amazon.com/cost-management/latest/userguide/what-is-costmanagement.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "AWS Cost Management - Analyzing your costs with AWS Cost Explorer",
                                      "url": "https://docs.aws.amazon.com/cost-management/latest/userguide/ce-what-is.html"
                                    },
                                    {
                                        "displayText": "Using Cost Explorer reports",
                                        "url": "https://docs.aws.amazon.com/cost-management/latest/userguide/ce-reports.html"
                                    },
                                    {
                                        "displayText": "Managing your costs with AWS Budgets",
                                        "url": "https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html"
                                    },
                                    {
                                      "displayText": "Detecting unusual spend with AWS Cost Anomaly Detection",
                                      "url": "https://docs.aws.amazon.com/cost-management/latest/userguide/manage-ad.html"
                                    }]
                                }
                            ]
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_10_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_10_1)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_11",
                    "title": "Do you have any cost attribution process of the IDP solution for your business to ensure accountability?",
                    "description": "How are you tracking and attributing AWS Costs to specific projects or LOBs to ensure accountability?",
                    "helpfulResource": {
                        "displayText": "Identify organization categories such as business units, departments, or projects that could be used to allocate cost within your organization to the internal consuming entities so that spend accountability can be enforced and consumption behaviors can be driven effectively.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_define_attribution.html"
                    },
                    "choices": [
                        {
                            "id": "COST_11_1",
                            "title": "We define AWS Cost Categories that map to our organization or functional categories.",
                            "helpfulResource": {
                                "displayText": "The process of categorizing costs is crucial in budgeting, accounting, financial reporting, decision making, benchmarking, and project management. By classifying and categorizing expenses, teams can gain a better understanding of the types of costs they will incur throughout their cloud journey, helping teams make informed decisions and manage budgets effectively.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_define_attribution.html"
                            },
                            "improvementPlan": {
                                "displayText": "Define your organization categories: Meet with stakeholders to define categories that reflect your organization's structure and requirements. These will directly map to the structure of existing financial categories, such as business unit, budget, cost center, or department. Multiple categories can be assigned to a resource, and a resource can be in multiple different categories, so define as many categories as needed.\nDefine your functional categories: Meet with stakeholders to define categories that reflect the functions that you have within your business. This may be the workload or application names, and the type of environment, such as production, testing, or development. Multiple categories can be assigned to a resource, and a resource can be in multiple different categories, so define as many categories as needed so that you can manage your costs within the categorized structure using AWS Cost Categories.\nDefine AWS Cost Categories: You can create cost categories to organize your cost and usage information. Use AWS Cost Categories to map your AWS costs and usage into meaningful categories. With cost categories, you can organize your costs using a rule-based engine. T",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_define_attribution.html"
                            },
                            "additionalResources":[
                                {
                                  "type": "IMPROVEMENT_PLAN",
                                  "content": [
                                    {
                                      "displayText": "Cloud Financial Management - AWS Cost Categories Features",
                                      "url": "https://aws.amazon.com/aws-cost-management/aws-cost-categories/features/"
                                    },
                                    {
                                        "displayText": "AWS Billing - Managing your costs with AWS Cost Categories",
                                        "url": "https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html"
                                    },
                                    {
                                        "displayText": "AWS Cloud Financial Management: Organize your cost and usage data with AWS Cost Categories",
                                        "url": "https://aws.amazon.com/blogs/aws-cloud-financial-management/organize-your-cost-and-usage-data-with-aws-cost-categories/"
                                    }]
                                }
                            ]
                        },
                        {
                            "id": "COST_11_2",
                            "title": "We define AWS Cost Categories that map to our organization or functional categories.",
                            "helpfulResource": {
                                "displayText": "The process of categorizing costs is crucial in budgeting, accounting, financial reporting, decision making, benchmarking, and project management. By classifying and categorizing expenses, teams can gain a better understanding of the types of costs they will incur throughout their cloud journey, helping teams make informed decisions and manage budgets effectively.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_define_attribution.html"
                            },
                            "improvementPlan": {
                                "displayText": "Define your organization categories: Meet with stakeholders to define categories that reflect your organization's structure and requirements. These will directly map to the structure of existing financial categories, such as business unit, budget, cost center, or department. Multiple categories can be assigned to a resource, and a resource can be in multiple different categories, so define as many categories as needed.\nDefine your functional categories: Meet with stakeholders to define categories that reflect the functions that you have within your business. This may be the workload or application names, and the type of environment, such as production, testing, or development. Multiple categories can be assigned to a resource, and a resource can be in multiple different categories, so define as many categories as needed so that you can manage your costs within the categorized structure using AWS Cost Categories.\nDefine AWS Cost Categories: You can create cost categories to organize your cost and usage information. Use AWS Cost Categories to map your AWS costs and usage into meaningful categories. With cost categories, you can organize your costs using a rule-based engine. T",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_monitor_usage_define_attribution.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_11_1 && COST_11_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "(!COST_11_1) || (!COST_11_2)",
                            "risk": "HIGH_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "COST_12",
                    "title": "How do you align expenditure to your business objectives and create Comprehend usage awareness ?",
                    "description": "How to easily identify Comprehend Analysis jobs, Custom classification models, Custom entity recognition models, and endpointsusage and costs? This helps measure return on investment (ROI) and gives workload owners an opportunity to optimize their resources and reduce costs.",
                    "helpfulResource": {
                        "displayText": "Ensure cost effective decisions are made with respect to long-term resource allocation.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlcost-01.html"
                    },
                    "choices": [
                        {
                            "id": "COST_13_1",
                            "title": "We use tags on ComprehendAnalysis jobs, Custom classification models, Custom entity recognition models, and endpoints.",
                            "helpfulResource": {
                                "displayText": "A tag is a key-value pair that you can add to an Amazon Comprehend resource as metadata.\nTags have two major functions: organizing your resources and providing tag-based access control.",
                                "url": "https://docs.aws.amazon.com/comprehend/latest/dg/tagging.html"
                            },
                            "improvementPlan": {
                                "displayText": "Assign resource tags to Amazon Comprehend. Tagging resources helps identify, track, and itemize their usage and costs.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/amazon-comprehend-now-supports-resource-tagging-for-custom-models/"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "COST_13_1",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                }
            ]
        },
        {
            "id": "sustainability",
            "name": "Sustainability",
            "questions": [
                {
                    "id": "SUS_1",
                    "title": "Do you set up your IDP workflow in a region that meets your business requirements and sustainability goals?",
                    "description": "To decide for the best regions for business requirements and sustainability goals, we recommend the following two steps: 1. Evaluate and shortlist potential regions for your workload based on your business requirements. 2. Select regions close to Amazon's renewable energy projects and regions listed as being powered by 100 percent renewable energy.",
                    "helpfulResource": {
                        "displayText": "When considering key business factors, you should evaluate latency, cost, available services and features and compliance.Based on the Greenhouse Gas (GHG) Protocol, there are two methods for tracking emissions from electricity production: market-based and location-based. Companies may choose one of these methods based on their relevant sustainability guidelines to track and compare their emissions year-to-year. Amazon uses themarket-based model to report our emissions.",
                        "url": "https://aws.amazon.com/blogs/architecture/how-to-select-a-region-for-your-workload-based-on-sustainability-goals/"
                    },
                    "choices": [
                        {
                            "id": "SUS_1_1",
                            "title": "Select a region that meets your business requirements.",
                            "helpfulResource": {
                                "displayText": "When considering key business factors, you should evaluate latency, cost, available services and features and compliance.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_region_a2.html"
                            },
                            "improvementPlan": {
                                "displayText": "Select a region that meets your key business requirements. Start by shortlisting potential regions for your workload based on your business requirements including compliance, available features, cost, and latency.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_region_a2.html"
                            }
                        },
                        {
                            "id": "SUS_1_2",
                            "title": "Select a region listed as powered by 100 percent renewable energy.",
                            "helpfulResource": {
                                "displayText": "Identify your relevant sustainability guidelines to track and compare year-to-year carbon emissions based on Greenhouse Gas Protocol (market-based and location based methods).\nChoose region based on method you use to track carbon emissions. Amazon is on a path to powering our operations with 100 percent renewable energy by 2025. In the below link you can find all regions which are already powered by 100 percent renewable energy",
                                "url": "https://sustainability.aboutamazon.com/products-services/the-cloud?energyType=true"
                            },
                            "improvementPlan": {
                                "displayText": "Choose region based on the method you use to track carbon emissions. Select a region listed as powered by 100 percent renewable energy.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sus_sus_region_a2.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SUS_1_1 && SUS_1_2",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SUS_2",
                    "title": "How do you manage your data and the lifecycle of your data to optimize your sustainability footprint?",
                    "description": "Managing your data and data lifecycle and using different storage tiers are key components to optimizing storage for sustainability.",
                    "helpfulResource": {
                        "displayText": "Storing and accessing data efficiently, in addition to reducing idle storage resources results in a more efficient and sustainable architecture. When you consider different storage mechanisms, remember that youre introducing a trade-off between resource efficiency, access latency, and reliability. This means youll need to select your management pattern accordingly.",
                        "url": "https://aws.amazon.com/blogs/architecture/optimizing-your-aws-infrastructure-for-sustainability-part-ii-storage/"
                    },
                    "choices": [
                        {
                            "id": "SUS_2_1",
                            "title": "Create and ingest only relevant data",
                            "helpfulResource": {
                                "displayText": "To optimize your storage footprint for sustainability, evaluate what data is needed to meet your business objectives and create and ingest only relevant data.",
                                "url": "https://aws.amazon.com/blogs/architecture/optimize-your-modern-data-architecture-for-sustainability-part-1-data-ingestion-and-data-lake/"
                            },
                            "improvementPlan": {
                                "displayText": "Create and ingest only relevant data.",
                                "url": "https://aws.amazon.com/blogs/architecture/optimize-your-modern-data-architecture-for-sustainability-part-1-data-ingestion-and-data-lake/"
                            }
                        },
                        {
                            "id": "SUS_2_2",
                            "title": "Ingest data at the optimal resolution for your business goals, rather than choosing the maximum resolution available.",
                            "helpfulResource": {
                                "displayText": "Amazon Textract requires at least 150 DPI. If your document is not in a supported Amazon Textract format (PDF, TIFF, JPEG, and PNG) and you need to convert it, experiment to find the optimal resolution for best results rather than choosing the maximum resolution.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/textract-best-practices.html"
                            },
                            "improvementPlan": {
                                "displayText": "Ingest data at the optimal resolution for your business goals, rather than choosing the maximum resolution available.",
                                "url": "https://docs.aws.amazon.com/textract/latest/dg/textract-best-practices.html"
                            }
                        },
                        {
                            "id": "SUS_2_3",
                            "title": "Store only relevant data",
                            "helpfulResource": {
                                "displayText": "Only store data that is relevant and not easily reproducible. In most IDP workflows, it is not necessary to store the data from each intermediate step because it is easy to reproduce. Continuously identify unused data and delete it.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlsec-07.html"
                            },
                            "improvementPlan": {
                                "displayText": "Preserve data across computing environments (such as development and staging) and store only data that is relevant to the use case.Implement mechanisms to enforce a lifecycle management process across the data. Decide when to automatically remove stale data.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlsec-07.html"
                            }
                        },
                        {
                            "id": "SUS_2_4",
                            "title": "Use the right technology to store data",
                            "helpfulResource": {
                                "displayText": "Using different tiers of storage is a key component of optimizing storage for sustainability. When considering different storage mechanisms, remember that you're making tradeoffs between resource efficiency, access latency, and reliability. That means you'll need to select your management pattern accordingly. By storing less volatile data on technologies designed for efficient long-term storage, you can optimize your storage footprint. For archiving data or storing slowly changing data, Amazon S3 Glacier and Amazon S3 Glacier Deep Archive are available. Depending on your data classification and workflow, you can choose Amazon S3 One Zone-IA, which reduces power and server capacity by storing data within a single availability zone. In general, you need to balance resource efficiency, access latency, and reliability when considering these storage mechanisms.",
                                "url": "https://aws.amazon.com/blogs/architecture/optimizing-your-aws-infrastructure-for-sustainability-part-ii-storage/"
                            },
                            "improvementPlan": {
                                "displayText": "Use the right technology to store data. Storing and accessing data efficiently results in a more sustainable architecture. Amazon CloudWatch offers storage metrics that can be used to assess storage improvements.",
                                "url": "https://aws.amazon.com/blogs/architecture/optimizing-your-aws-infrastructure-for-sustainability-part-ii-storage/"
                            }
                        },
                        {
                            "id": "SUS_2_5",
                            "title": "Actively manage your data lifecycle according to your sustainability goals",
                            "helpfulResource": {
                                "displayText": "Managing your data lifecycle means optimizing your storage footprint.To store your data efficiently throughout its lifetime, create Amazon S3 Lifecycle configurations that automatically transfers objects to a different storage class based on your pre-defined rules.For data with unknown or changing access patterns, use Amazon S3 Intelligent-Tiering to monitor access patterns and move objects among tiers automatically. In general, you have to make a trade-off between resource efficiency, access latency, and reliability when considering these storage mechanisms.",
                                "url": "https://aws.amazon.com/blogs/architecture/optimizing-your-aws-infrastructure-for-sustainability-part-ii-storage/"
                            },
                            "improvementPlan": {
                                "displayText": "Actively manage your data lifecycle according to your sustainability goals. UseAmazon S3 Lifecycle configurations that automatically transfers objects to a different storage class based on your pre-defined rules. Consider expiring Amazon S3 objects based on last accessed date.",
                                "url": "https://aws.amazon.com/blogs/architecture/optimizing-your-aws-infrastructure-for-sustainability-part-ii-storage/"
                            }
                        },
                        {
                            "id": "SUS_2_6",
                            "title": "Continuously optimize your storage footprint by using the right tools",
                            "helpfulResource": {
                                "displayText": "Tools likeAmazon S3 Storage Lens delivers visibility into storage usage, activity trends, and even makes recommendations for improvements. This information can be used to lower the environmental impact of storing data.",
                                "url": "https://aws.amazon.com/s3/storage-analytics-insights/"
                            },
                            "improvementPlan": {
                                "displayText": "Leverage tools like Amazon S3 Storage Lens to continuously analyze and optimize your storage footprint.",
                                "url": "https://aws.amazon.com/s3/storage-analytics-insights/"
                            }
                        },
                        {
                            "id": "SUS_2_7",
                            "title": "Enable data and compute proximity",
                            "helpfulResource": {
                                "displayText": "As you make your IDP workflow available to more customers, the amount of data traveling over the network will increase. Similarly, the larger the size of the data and the greater the distance a packet must travel, the more resources are required to transmit it. Reducing the amount of data sent over the network and optimizing the path a packet takes will result in more efficient data transfer. Setting up data storage closely to data processing helps optimize sustainability at the network layer.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlcost-21.html"
                            },
                            "improvementPlan": {
                                "displayText": "Ensure that the region used to store the data is the same Region where ou have deployed your IDP workflow. This approach helps minimize the time and cost of transferring data to the computing environment.",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlcost-21.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SUS_2_1 && SUS_2_2 && SUS_2_3 && SUS_2_4 && SUS_2_5 && SUS_2_6 && SUS_2_7",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SUS_3",
                    "title": "Do you re-evaluate your IDP workflow when a new feature is released in a related service?",
                    "description": "Keeping your IDP workloads up to date will help you reduce the sustainability impact and gain performance efficiencies. There are several blogs and resources available to help you stay on top of AWS announcements.",
                    "helpfulResource": {
                        "displayText": "Keep your workload up-to-date to adopt efficient features, remove issues, and improve the overall efficiency of your workload.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_dev_a3.html"
                    },
                    "choices": [
                        {
                            "id": "SUS_3_1",
                            "title": "Use AWS re:post to stay informed about feature updates",
                            "helpfulResource": {
                                "displayText": "re:Post is a community-driven, questions-and-answers service designed to help AWS customers remove technical roadblocks, accelerate innovation, and enhance operations. AWS re:Post has 40+ topics including a community dedicated to AWS Well-Architected.",
                                "url": "https://repost.aws/"
                            },
                            "improvementPlan": {
                                "displayText": "Use AWS re:post to stay informed about feature updates and get support for improving sustainability in a community dedicated to AWS Well-Architected.",
                                "url": "https://aws.amazon.com/about-aws/whats-new/2022/05/aws-community-well-architected-tool/"
                            }
                        },
                        {
                            "id": "SUS_3_2",
                            "title": "Use AWS Blogs to stay up to date for Amazon Textract",
                            "helpfulResource": {
                                "displayText": "AWS blogs help customers stay up-to-date on the most important services they use. To stay up to date on Amazon Textract, check out the Machine Learning blog.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/"
                            },
                            "improvementPlan": {
                                "displayText": "Use AWS Blogs to stay up to date for Amazon Textract",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_dev_a3.html"
                            }
                        },
                        {
                            "id": "SUS_3_3",
                            "title": "Use AWS Blogs to stay up to date for Amazon Comprehend",
                            "helpfulResource": {
                                "displayText": "AWS blogs help customers stay up-to-date on the most important services they use. To stay up to date on Amazon Comprehend, check out the Machine Learning blog.",
                                "url": "https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/"
                            },
                            "improvementPlan": {
                                "displayText": "Use AWS Blogs to stay up to date for Amazon Comprehend",
                                "url": "https://docs.aws.amazon.com/wellarchitected/latest/framework/sus_sus_dev_a3.html"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SUS_3_1 && SUS_3_2 && SUS_3_3",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                },
                {
                    "id": "SUS_4",
                    "title": "How do you design your IDP solution for continuous improvement?",
                    "description": "Creating a flexible, extensible architecture for your IDP solution enables you to optimize resource usage over time.",
                    "helpfulResource": {
                        "displayText": "Improving sustainability is a continuous process that requires flexible architectures and automation to support frequent improvements. When your architecture is loosely coupled and leverages serverless and managed services, it is easy to enable new features and replace components for increased sustainability.",
                        "url": "https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/improvement-process.html"
                    },
                    "choices": [
                        {
                            "id": "SUS_4_1",
                            "title": "Improve safely and continuously through automation",
                            "helpfulResource": {
                                "displayText": "Using automation to deploy all changes reduces the potential for human error and provides the ability to test before making production changes to ensure your plans are complete. For example, automate your software delivery process using continuous integration and delivery (CI/CD) pipelines to test and deploy potential improvements to reduce effort and limit errors caused by manual processes. Define changes using infrastructure as code: All configurations should be defined declaratively and stored in a source control system like AWS CodeCommit, just like application code. Infrastructure provisioning, orchestration, and deployment should also support infrastructure as code.",
                                "url": "https://aws.amazon.com/getting-started/hands-on/set-up-ci-cd-pipeline/"
                            },
                            "improvementPlan": {
                                "displayText": "Improve safely and continuously through automation.Automate your software delivery process using continuous integration and delivery (CI/CD) pipelines.",
                                "url": "https://aws.amazon.com/getting-started/hands-on/set-up-ci-cd-pipeline/"
                            }
                        },
                        {
                            "id": "SUS_4_2",
                            "title": "Use an event-driven architecture",
                            "helpfulResource": {
                                "displayText": "Using AWS serverless services to implement an event-driven approach will allow you to build scalable, fault-tolerant applications. You can use messaging services such as Amazon SQS for reliable and durable communication between microservices. For event fan-out, you can use Amazon SNS topics. If you need event filtering and routing, you can use Amazon EventBridge. This will help to minimize idle resources and improve sustainability.",
                                "url": "https://aws.amazon.com/event-driven-architecture/"
                            },
                            "improvementPlan": {
                                "displayText": "Use an event-driven architecture.",
                                "url": "https://aws.amazon.com/event-driven-architecture/"
                            }
                        },
                        {
                            "id": "SUS_4_3",
                            "title": "Use serverless services for workflow orchestration",
                            "helpfulResource": {
                                "displayText": "Serverless services help provide the mechanism to build a solution for IDP quickly and sustainably. Services such as AWS Lambda, AWS Step Functions, and Amazon EventBridge help to orchestrate your workflow driven by events and minimize idle resources to improve sustainability.",
                                "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/orchestration-choreography.html"
                            },
                            "improvementPlan": {
                                "displayText": "Use serverless services like AWS Lambda, Amazon EventBridge or AWS Step Functions for orchestrating your workflow.",
                                "url": "https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/orchestration-choreography.html"
                            }
                        },
                        {
                            "id": "SUS_4_4",
                            "title": "Use managed services like Textract and Comprehend",
                            "helpfulResource": {
                                "displayText": "IDP can be performed using a custom model or managed services such as Amazon Textract and Amazon Comprehend, reducing the effort required to develop and retrain your service. By using managed services instead of your own custom model, you can reduce the amount of energy required to train a model.",
                                "url": "https://aws.amazon.com/solutions/guidance/intelligent-document-processing-on-aws/"
                            },
                            "improvementPlan": {
                                "displayText": "Use managed services like Textract and Comprehend.",
                                "url": "https://aws.amazon.com/solutions/guidance/intelligent-document-processing-on-aws/"
                            }
                        }
                    ],
                    "riskRules": [
                        {
                            "condition": "SUS_4_1 && SUS_4_2 && SUS_4_3 && SUS_4_4",
                            "risk": "NO_RISK"
                        },
                        {
                            "condition": "default",
                            "risk": "MEDIUM_RISK"
                        }
                    ]
                }
            ]
        }
    ]
}